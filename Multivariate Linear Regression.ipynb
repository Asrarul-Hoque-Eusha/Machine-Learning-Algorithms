{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07946740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d515d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>beds</th>\n",
       "      <th>bath</th>\n",
       "      <th>area</th>\n",
       "      <th>adress</th>\n",
       "      <th>type</th>\n",
       "      <th>purpose</th>\n",
       "      <th>flooPlan</th>\n",
       "      <th>url</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eminent Apartment Of 2200 Sq Ft Is Vacant For ...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2,200 sqft</td>\n",
       "      <td>Block A, Bashundhara R-A, Dhaka</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>For Rent</td>\n",
       "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
       "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
       "      <td>August 13, 2022</td>\n",
       "      <td>50 Thousand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apartment Ready To Rent In South Khulshi, Near...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1,400 sqft</td>\n",
       "      <td>South Khulshi, Khulshi, Chattogram</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>For Rent</td>\n",
       "      <td>https://images-cdn.bproperty.com/thumbnails/44...</td>\n",
       "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
       "      <td>January 25, 2022</td>\n",
       "      <td>30 Thousand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smartly priced 1950 SQ FT apartment, that you ...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1,950 sqft</td>\n",
       "      <td>Block F, Bashundhara R-A, Dhaka</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>For Rent</td>\n",
       "      <td>https://images-cdn.bproperty.com/thumbnails/11...</td>\n",
       "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
       "      <td>February 22, 2023</td>\n",
       "      <td>30 Thousand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000 Sq Ft Residential Apartment Is Up For Ren...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2,000 sqft</td>\n",
       "      <td>Sector 9, Uttara, Dhaka</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>For Rent</td>\n",
       "      <td>https://images-cdn.bproperty.com/thumbnails/14...</td>\n",
       "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
       "      <td>October 28, 2021</td>\n",
       "      <td>35 Thousand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strongly Structured This 1650 Sq. ft Apartment...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1,650 sqft</td>\n",
       "      <td>Block I, Bashundhara R-A, Dhaka</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>For Rent</td>\n",
       "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
       "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
       "      <td>February 19, 2023</td>\n",
       "      <td>25 Thousand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title beds bath        area   \n",
       "0  Eminent Apartment Of 2200 Sq Ft Is Vacant For ...   3    4   2,200 sqft  \\\n",
       "1  Apartment Ready To Rent In South Khulshi, Near...   3    4   1,400 sqft   \n",
       "2  Smartly priced 1950 SQ FT apartment, that you ...   3    4   1,950 sqft   \n",
       "3  2000 Sq Ft Residential Apartment Is Up For Ren...   3    3   2,000 sqft   \n",
       "4  Strongly Structured This 1650 Sq. ft Apartment...   3    4   1,650 sqft   \n",
       "\n",
       "                               adress       type   purpose   \n",
       "0     Block A, Bashundhara R-A, Dhaka  Apartment  For Rent  \\\n",
       "1  South Khulshi, Khulshi, Chattogram  Apartment  For Rent   \n",
       "2     Block F, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
       "3             Sector 9, Uttara, Dhaka  Apartment  For Rent   \n",
       "4     Block I, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
       "\n",
       "                                            flooPlan   \n",
       "0  https://images-cdn.bproperty.com/thumbnails/10...  \\\n",
       "1  https://images-cdn.bproperty.com/thumbnails/44...   \n",
       "2  https://images-cdn.bproperty.com/thumbnails/11...   \n",
       "3  https://images-cdn.bproperty.com/thumbnails/14...   \n",
       "4  https://images-cdn.bproperty.com/thumbnails/10...   \n",
       "\n",
       "                                                 url        lastUpdated   \n",
       "0  https://www.bproperty.com/en/property/details-...    August 13, 2022  \\\n",
       "1  https://www.bproperty.com/en/property/details-...   January 25, 2022   \n",
       "2  https://www.bproperty.com/en/property/details-...  February 22, 2023   \n",
       "3  https://www.bproperty.com/en/property/details-...   October 28, 2021   \n",
       "4  https://www.bproperty.com/en/property/details-...  February 19, 2023   \n",
       "\n",
       "         price  \n",
       "0  50 Thousand  \n",
       "1  30 Thousand  \n",
       "2  30 Thousand  \n",
       "3  35 Thousand  \n",
       "4  25 Thousand  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"property_listing_data_in_Bangladesh.csv\")\n",
    "df.head(5)\n",
    "#print(df['type'].loc[df.index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1bbfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  beds bath        area        price\n",
      "0   3    4   2,200 sqft  50 Thousand\n",
      "1   3    4   1,400 sqft  30 Thousand\n",
      "2   3    4   1,950 sqft  30 Thousand\n",
      "3   3    3   2,000 sqft  35 Thousand\n",
      "4   3    4   1,650 sqft  25 Thousand\n",
      "(7557, 4)\n"
     ]
    }
   ],
   "source": [
    "n=df.shape[1]\n",
    "#df=df[df['type'].index!=\"Apartment\"]\n",
    "#removing Building and Duplex type\n",
    "df.drop(df.loc[df['type']==0].index, inplace=True)\n",
    "#removing extra string type data\n",
    "df=df.drop(['title','adress','type','purpose','flooPlan','url','lastUpdated'],axis=1)\n",
    "print(df.head(5))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac796691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beds     object\n",
       "bath     object\n",
       "area     object\n",
       "price    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821747f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beds</th>\n",
       "      <th>bath</th>\n",
       "      <th>area</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2200</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1400</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1950</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>35000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1650</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3400</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1600</td>\n",
       "      <td>35000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1250</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2150</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1250</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  beds bath  area    price\n",
       "0    3    4  2200    50000\n",
       "1    3    4  1400    30000\n",
       "2    3    4  1950    30000\n",
       "3    3    3  2000    35000\n",
       "4    3    4  1650    25000\n",
       "5    5    5  3400  1100000\n",
       "6    3    3  1600    35000\n",
       "7    3    3  1250    23000\n",
       "8    3    4  2150    40000\n",
       "9    3    3  1250    23000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Lakh and thousand\n",
    "n,m=df.shape\n",
    "ts=\"0123456789\"\n",
    "for i in range(n):\n",
    "    s=df.beds[i]\n",
    "    for j in s:\n",
    "        if(j not in ts):\n",
    "            s=s.replace(j,\"\")\n",
    "    df.beds[i]=int(s)\n",
    "    s=df.bath[i]\n",
    "    #print(s)\n",
    "    for j in s:\n",
    "        if(j not in ts):\n",
    "            s=s.replace(j,\"\")\n",
    "    df.bath[i]=int(s)\n",
    "    #print(s)\n",
    "    s=df.area[i]\n",
    "    for j in s:\n",
    "        if(j not in ts):\n",
    "            s=s.replace(j,\"\")\n",
    "    df.area[i]=int(s)\n",
    "    #print(s)\n",
    "    s=df.price[i]\n",
    "    st=\"Thousand\"\n",
    "    sl=\"Lakh\"\n",
    "    if(st in s):\n",
    "        a=1000\n",
    "    if(sl in s):\n",
    "        a=100000\n",
    "    for j in s:\n",
    "        if(j not in ts):\n",
    "            s=s.replace(j,\"\")\n",
    "    df.price[i]=int(s)*a\n",
    "    \n",
    "#df['beds']=pd.to_int(df['beds'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3c0531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372.0250099245732\n",
      "956.6929589970397\n",
      "DF Area\n",
      "  beds bath      area  price\n",
      "0    3    4  0.865455  50000\n",
      "1    3    4  0.029241  30000\n",
      "2    3    4  0.604138  30000\n",
      "3    3    3  0.656402  35000\n"
     ]
    }
   ],
   "source": [
    "marea=np.mean(df['area'])   #Z scor normalization\n",
    "stdarea=np.std(df['area'])\n",
    "df['area']=(df['area']-marea)/stdarea\n",
    "print(marea)\n",
    "print(stdarea)\n",
    "print(f\"DF Area\\n{df.head(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74e03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mprice=np.mean(df['price'])         #Z scor normalization\n",
    "#stdprice=np.std(df['price'])\n",
    "#df['price']=(df['price']-mprice)/stdprice\n",
    "#print(mprice)\n",
    "#print(stdprice)\n",
    "#print(f\"DF Price\\n{df.head(20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d4495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(W,b,X):\n",
    "    m,_=X.shape\n",
    "    f_wb=np.zeros(m)\n",
    "    f_wb=f_wb.reshape(m,1)\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb[i]=np.dot(X.iloc[i],W)+b\n",
    "    \n",
    "    #f_wb=f_wb.reshape((1,m))\n",
    "    return f_wb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac0450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost_Function(W,b,X,Y):\n",
    "    f_wb=Predict(W,b,X)\n",
    "    m,_=X.shape\n",
    "    cost=0\n",
    "    #for i in range(m):\n",
    "    cost=np.sum((f_wb-Y)**2)\n",
    "    totcost=cost/(2*m)\n",
    "    return totcost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02587b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(W,b,X,Y):\n",
    "    \n",
    "    m,n=X.shape\n",
    "    db=0.\n",
    "    dw=np.zeros((n,1))\n",
    "    f_wb=Predict(W,b,X)\n",
    "    #lit=['beds','bath','area']\n",
    "    db=np.sum(f_wb-Y)\n",
    "    dw=np.dot(X.T,(f_wb-Y))\n",
    "    #for i in range(m):\n",
    "        #W=W-alpha*((1/m)*np.sum(f_wb-Y),axis=0)\n",
    "     #   for j in range(n):\n",
    "      #      dw[j]+=(f_wb[i]-Y.iloc[i])*X.iloc[i, j]\n",
    "       # db+=(f_wb[i]-Y.iloc[i])\n",
    "    db=db/m\n",
    "    dw=dw/m\n",
    "    \n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af6f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Descent(W_in,b_in,X,Y,X_v,Y_v,num_iteration,alpha):\n",
    "    \n",
    "    #J_history=[]\n",
    "    W=W_in\n",
    "    b=b_in\n",
    "    \n",
    "    for i in range(num_iteration):\n",
    "        dw,db=Gradient(W,b,X,Y)\n",
    "        \n",
    "        W=W-alpha*dw\n",
    "        b=b-alpha*db\n",
    "        \n",
    "        #J_history.append(Cost_Function(W,b,X,Y))\n",
    "        train_loss=Cost_Function(W,b,X,Y)\n",
    "        valid_loss=Cost_Function(W,b,X_v,Y_v)\n",
    "        print(f\"Iteration {i:4d}  ->  Training Loss: {train_loss.item():8.2f}     Validation Loss: {valid_loss.item():0.2f}\")\n",
    "            \n",
    "            \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "334ef08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  beds bath      area\n",
      "0    3    4  0.865455\n",
      "1    3    4  0.029241\n",
      "2    3    4  0.604138\n",
      "3    3    3  0.656402\n",
      "4    3    4  0.290558\n",
      "   price\n",
      "0  50000\n",
      "1  30000\n",
      "2  30000\n",
      "3  35000\n",
      "4  25000\n"
     ]
    }
   ],
   "source": [
    "X=df[['beds','bath','area']]\n",
    "Y=df[['price']]\n",
    "print(X.head(5))\n",
    "print(Y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a023ad1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1         2\n",
      "0  3  4  0.865455\n",
      "1  3  4  0.029241\n",
      "2  3  4  0.604138\n",
      "3  3  3  0.656402\n",
      "4  3  4  0.290558\n",
      "5  5  5  2.119776\n",
      "6  3  3  0.238295\n",
      "7  3  3 -0.127549\n",
      "8  3  4  0.813192\n",
      "9  3  3 -0.127549\n",
      "(151, 3)\n",
      "(151, 1)\n",
      "(222, 3)\n",
      "(222, 1)\n",
      "(359, 3)\n",
      "(359, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11040\\1477788151.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.rename(columns={X.columns[0]: 0},inplace=True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11040\\1477788151.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.rename(columns={X.columns[1]: 1},inplace=True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11040\\1477788151.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.rename(columns={X.columns[2]: 2},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X.rename(columns={X.columns[0]: 0},inplace=True)\n",
    "X.rename(columns={X.columns[1]: 1},inplace=True)\n",
    "X.rename(columns={X.columns[2]: 2},inplace=True)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.98,random_state=42)\n",
    "X_valid,X_test,Y_valid,Y_test=train_test_split(X_test,Y_test,test_size=0.97,random_state=42)\n",
    "X_test,x,Y_test,y=train_test_split(X_test,Y_test,test_size=0.95,random_state=42)\n",
    "\n",
    "\n",
    "print(X.head(10))\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(Y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b839785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81574482]\n",
      " [0.91558575]\n",
      " [0.68562294]]\n",
      "Iteration    0  ->  Training Loss: 97863759676.63     Validation Loss: 64996670156.82\n",
      "Iteration    1  ->  Training Loss: 94627336790.37     Validation Loss: 62643509356.12\n",
      "Iteration    2  ->  Training Loss: 92236707472.69     Validation Loss: 61148347964.62\n",
      "Iteration    3  ->  Training Loss: 90423891517.05     Validation Loss: 60213529749.37\n",
      "Iteration    4  ->  Training Loss: 89006719152.54     Validation Loss: 59640781696.02\n",
      "Iteration    5  ->  Training Loss: 87861338943.19     Validation Loss: 59298741594.57\n",
      "Iteration    6  ->  Training Loss: 86903533888.62     Validation Loss: 59100984972.12\n",
      "Iteration    7  ->  Training Loss: 86076023660.41     Validation Loss: 58991173531.50\n",
      "Iteration    8  ->  Training Loss: 85339835072.09     Validation Loss: 58933031418.80\n",
      "Iteration    9  ->  Training Loss: 84668437358.71     Validation Loss: 58903592204.57\n",
      "Iteration   10  ->  Training Loss: 84043756446.95     Validation Loss: 58888659817.95\n",
      "Iteration   11  ->  Training Loss: 83453466204.64     Validation Loss: 58879766522.68\n",
      "Iteration   12  ->  Training Loss: 82889147537.81     Validation Loss: 58872141817.01\n",
      "Iteration   13  ->  Training Loss: 82345037284.88     Validation Loss: 58863362836.98\n",
      "Iteration   14  ->  Training Loss: 81817177942.87     Validation Loss: 58852463202.84\n",
      "Iteration   15  ->  Training Loss: 81302839802.82     Validation Loss: 58839349416.70\n",
      "Iteration   16  ->  Training Loss: 80800128217.27     Validation Loss: 58824422865.42\n",
      "Iteration   17  ->  Training Loss: 80307716685.28     Validation Loss: 58808338659.46\n",
      "Iteration   18  ->  Training Loss: 79824665444.53     Validation Loss: 58791855011.51\n",
      "Iteration   19  ->  Training Loss: 79350298174.87     Validation Loss: 58775742066.06\n",
      "Iteration   20  ->  Training Loss: 78884118195.14     Validation Loss: 58760729371.17\n",
      "Iteration   21  ->  Training Loss: 78425751500.18     Validation Loss: 58747478122.30\n",
      "Iteration   22  ->  Training Loss: 77974908038.76     Validation Loss: 58736568982.56\n",
      "Iteration   23  ->  Training Loss: 77531355388.44     Validation Loss: 58728499425.93\n",
      "Iteration   24  ->  Training Loss: 77094900855.57     Validation Loss: 58723686655.55\n",
      "Iteration   25  ->  Training Loss: 76665379301.36     Validation Loss: 58722473554.70\n",
      "Iteration   26  ->  Training Loss: 76242644859.37     Validation Loss: 58725136061.60\n",
      "Iteration   27  ->  Training Loss: 75826565298.06     Validation Loss: 58731890975.42\n",
      "Iteration   28  ->  Training Loss: 75417018180.84     Validation Loss: 58742903603.78\n",
      "Iteration   29  ->  Training Loss: 75013888248.06     Validation Loss: 58758294922.88\n",
      "Iteration   30  ->  Training Loss: 74617065629.34     Validation Loss: 58778148087.17\n",
      "Iteration   31  ->  Training Loss: 74226444620.66     Validation Loss: 58802514228.88\n",
      "Iteration   32  ->  Training Loss: 73841922845.04     Validation Loss: 58831417549.73\n",
      "Iteration   33  ->  Training Loss: 73463400674.33     Validation Loss: 58864859742.68\n",
      "Iteration   34  ->  Training Loss: 73090780828.40     Validation Loss: 58902823799.86\n",
      "Iteration   35  ->  Training Loss: 72723968095.13     Validation Loss: 58945277270.51\n",
      "Iteration   36  ->  Training Loss: 72362869132.52     Validation Loss: 58992175033.59\n",
      "Iteration   37  ->  Training Loss: 72007392326.83     Validation Loss: 59043461647.51\n",
      "Iteration   38  ->  Training Loss: 71657447688.80     Validation Loss: 59099073334.15\n",
      "Iteration   39  ->  Training Loss: 71312946776.03     Validation Loss: 59158939649.10\n",
      "Iteration   40  ->  Training Loss: 70973802633.06     Validation Loss: 59222984883.85\n",
      "Iteration   41  ->  Training Loss: 70639929743.72     Validation Loss: 59291129240.17\n",
      "Iteration   42  ->  Training Loss: 70311243991.96     Validation Loss: 59363289811.43\n",
      "Iteration   43  ->  Training Loss: 69987662628.38     Validation Loss: 59439381400.67\n",
      "Iteration   44  ->  Training Loss: 69669104240.95     Validation Loss: 59519317201.25\n",
      "Iteration   45  ->  Training Loss: 69355488728.52     Validation Loss: 59603009361.67\n",
      "Iteration   46  ->  Training Loss: 69046737276.45     Validation Loss: 59690369453.24\n",
      "Iteration   47  ->  Training Loss: 68742772333.68     Validation Loss: 59781308856.09\n",
      "Iteration   48  ->  Training Loss: 68443517590.96     Validation Loss: 59875739077.00\n",
      "Iteration   49  ->  Training Loss: 68148897959.92     Validation Loss: 59973572009.88\n",
      "Iteration   50  ->  Training Loss: 67858839552.84     Validation Loss: 60074720148.63\n",
      "Iteration   51  ->  Training Loss: 67573269662.98     Validation Loss: 60179096759.92\n",
      "Iteration   52  ->  Training Loss: 67292116745.34     Validation Loss: 60286616022.76\n",
      "Iteration   53  ->  Training Loss: 67015310397.93     Validation Loss: 60397193140.28\n",
      "Iteration   54  ->  Training Loss: 66742781343.30     Validation Loss: 60510744428.39\n",
      "Iteration   55  ->  Training Loss: 66474461410.50     Validation Loss: 60627187385.19\n",
      "Iteration   56  ->  Training Loss: 66210283517.32     Validation Loss: 60746440744.42\n",
      "Iteration   57  ->  Training Loss: 65950181652.89     Validation Loss: 60868424515.63\n",
      "Iteration   58  ->  Training Loss: 65694090860.47     Validation Loss: 60993060013.41\n",
      "Iteration   59  ->  Training Loss: 65441947220.66     Validation Loss: 61120269877.40\n",
      "Iteration   60  ->  Training Loss: 65193687834.75     Validation Loss: 61249978084.93\n",
      "Iteration   61  ->  Training Loss: 64949250808.47     Validation Loss: 61382109957.36\n",
      "Iteration   62  ->  Training Loss: 64708575235.89     Validation Loss: 61516592161.34\n",
      "Iteration   63  ->  Training Loss: 64471601183.64     Validation Loss: 61653352705.96\n",
      "Iteration   64  ->  Training Loss: 64238269675.42     Validation Loss: 61792320936.37\n",
      "Iteration   65  ->  Training Loss: 64008522676.64     Validation Loss: 61933427524.74\n",
      "Iteration   66  ->  Training Loss: 63782303079.41     Validation Loss: 62076604458.87\n",
      "Iteration   67  ->  Training Loss: 63559554687.77     Validation Loss: 62221785029.02\n",
      "Iteration   68  ->  Training Loss: 63340222203.07     Validation Loss: 62368903813.35\n",
      "Iteration   69  ->  Training Loss: 63124251209.65     Validation Loss: 62517896662.12\n",
      "Iteration   70  ->  Training Loss: 62911588160.79     Validation Loss: 62668700681.10\n",
      "Iteration   71  ->  Training Loss: 62702180364.74     Validation Loss: 62821254214.18\n",
      "Iteration   72  ->  Training Loss: 62495975971.15     Validation Loss: 62975496825.60\n",
      "Iteration   73  ->  Training Loss: 62292923957.57     Validation Loss: 63131369281.72\n",
      "Iteration   74  ->  Training Loss: 62092974116.27     Validation Loss: 63288813532.59\n",
      "Iteration   75  ->  Training Loss: 61896077041.21     Validation Loss: 63447772693.32\n",
      "Iteration   76  ->  Training Loss: 61702184115.21     Validation Loss: 63608191025.45\n",
      "Iteration   77  ->  Training Loss: 61511247497.40     Validation Loss: 63770013918.24\n",
      "Iteration   78  ->  Training Loss: 61323220110.78     Validation Loss: 63933187870.08\n",
      "Iteration   79  ->  Training Loss: 61138055630.06     Validation Loss: 64097660469.93\n",
      "Iteration   80  ->  Training Loss: 60955708469.61     Validation Loss: 64263380378.90\n",
      "Iteration   81  ->  Training Loss: 60776133771.66     Validation Loss: 64430297312.05\n",
      "Iteration   82  ->  Training Loss: 60599287394.70     Validation Loss: 64598362020.27\n",
      "Iteration   83  ->  Training Loss: 60425125902.01     Validation Loss: 64767526272.44\n",
      "Iteration   84  ->  Training Loss: 60253606550.41     Validation Loss: 64937742837.75\n",
      "Iteration   85  ->  Training Loss: 60084687279.21     Validation Loss: 65108965468.28\n",
      "Iteration   86  ->  Training Loss: 59918326699.25     Validation Loss: 65281148881.79\n",
      "Iteration   87  ->  Training Loss: 59754484082.25     Validation Loss: 65454248744.76\n",
      "Iteration   88  ->  Training Loss: 59593119350.19     Validation Loss: 65628221655.66\n",
      "Iteration   89  ->  Training Loss: 59434193064.96     Validation Loss: 65803025128.48\n",
      "Iteration   90  ->  Training Loss: 59277666418.14     Validation Loss: 65978617576.52\n",
      "Iteration   91  ->  Training Loss: 59123501220.90     Validation Loss: 66154958296.39\n",
      "Iteration   92  ->  Training Loss: 58971659894.18     Validation Loss: 66332007452.30\n",
      "Iteration   93  ->  Training Loss: 58822105458.88     Validation Loss: 66509726060.55\n",
      "Iteration   94  ->  Training Loss: 58674801526.31     Validation Loss: 66688075974.35\n",
      "Iteration   95  ->  Training Loss: 58529712288.77     Validation Loss: 66867019868.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   96  ->  Training Loss: 58386802510.23     Validation Loss: 67046521226.07\n",
      "Iteration   97  ->  Training Loss: 58246037517.26     Validation Loss: 67226544321.11\n",
      "Iteration   98  ->  Training Loss: 58107383189.98     Validation Loss: 67407054207.14\n",
      "Iteration   99  ->  Training Loss: 57970805953.28     Validation Loss: 67588016701.77\n",
      "Iteration  100  ->  Training Loss: 57836272768.05     Validation Loss: 67769398373.15\n",
      "Iteration  101  ->  Training Loss: 57703751122.70     Validation Loss: 67951166526.40\n",
      "Iteration  102  ->  Training Loss: 57573209024.67     Validation Loss: 68133289190.31\n",
      "Iteration  103  ->  Training Loss: 57444614992.16     Validation Loss: 68315735104.15\n",
      "Iteration  104  ->  Training Loss: 57317938045.99     Validation Loss: 68498473704.83\n",
      "Iteration  105  ->  Training Loss: 57193147701.55     Validation Loss: 68681475114.17\n",
      "Iteration  106  ->  Training Loss: 57070213960.90     Validation Loss: 68864710126.49\n",
      "Iteration  107  ->  Training Loss: 56949107305.04     Validation Loss: 69048150196.28\n",
      "Iteration  108  ->  Training Loss: 56829798686.19     Validation Loss: 69231767426.19\n",
      "Iteration  109  ->  Training Loss: 56712259520.32     Validation Loss: 69415534555.17\n",
      "Iteration  110  ->  Training Loss: 56596461679.72     Validation Loss: 69599424946.81\n",
      "Iteration  111  ->  Training Loss: 56482377485.72     Validation Loss: 69783412577.91\n",
      "Iteration  112  ->  Training Loss: 56369979701.51     Validation Loss: 69967472027.19\n",
      "Iteration  113  ->  Training Loss: 56259241525.09     Validation Loss: 70151578464.24\n",
      "Iteration  114  ->  Training Loss: 56150136582.32     Validation Loss: 70335707638.64\n",
      "Iteration  115  ->  Training Loss: 56042638920.10     Validation Loss: 70519835869.27\n",
      "Iteration  116  ->  Training Loss: 55936722999.62     Validation Loss: 70703940033.78\n",
      "Iteration  117  ->  Training Loss: 55832363689.78     Validation Loss: 70887997558.25\n",
      "Iteration  118  ->  Training Loss: 55729536260.63     Validation Loss: 71071986407.07\n",
      "Iteration  119  ->  Training Loss: 55628216377.01     Validation Loss: 71255885072.90\n",
      "Iteration  120  ->  Training Loss: 55528380092.22     Validation Loss: 71439672566.91\n",
      "Iteration  121  ->  Training Loss: 55430003841.82     Validation Loss: 71623328409.07\n",
      "Iteration  122  ->  Training Loss: 55333064437.53     Validation Loss: 71806832618.72\n",
      "Iteration  123  ->  Training Loss: 55237539061.20     Validation Loss: 71990165705.23\n",
      "Iteration  124  ->  Training Loss: 55143405258.95     Validation Loss: 72173308658.82\n",
      "Iteration  125  ->  Training Loss: 55050640935.28     Validation Loss: 72356242941.57\n",
      "Iteration  126  ->  Training Loss: 54959224347.42     Validation Loss: 72538950478.58\n",
      "Iteration  127  ->  Training Loss: 54869134099.63     Validation Loss: 72721413649.25\n",
      "Iteration  128  ->  Training Loss: 54780349137.72     Validation Loss: 72903615278.73\n",
      "Iteration  129  ->  Training Loss: 54692848743.54     Validation Loss: 73085538629.52\n",
      "Iteration  130  ->  Training Loss: 54606612529.65     Validation Loss: 73267167393.23\n",
      "Iteration  131  ->  Training Loss: 54521620434.02     Validation Loss: 73448485682.42\n",
      "Iteration  132  ->  Training Loss: 54437852714.86     Validation Loss: 73629478022.65\n",
      "Iteration  133  ->  Training Loss: 54355289945.44     Validation Loss: 73810129344.64\n",
      "Iteration  134  ->  Training Loss: 54273913009.15     Validation Loss: 73990424976.55\n",
      "Iteration  135  ->  Training Loss: 54193703094.46     Validation Loss: 74170350636.42\n",
      "Iteration  136  ->  Training Loss: 54114641690.11     Validation Loss: 74349892424.69\n",
      "Iteration  137  ->  Training Loss: 54036710580.30     Validation Loss: 74529036816.95\n",
      "Iteration  138  ->  Training Loss: 53959891839.94     Validation Loss: 74707770656.70\n",
      "Iteration  139  ->  Training Loss: 53884167830.03     Validation Loss: 74886081148.29\n",
      "Iteration  140  ->  Training Loss: 53809521193.12     Validation Loss: 75063955849.99\n",
      "Iteration  141  ->  Training Loss: 53735934848.75     Validation Loss: 75241382667.17\n",
      "Iteration  142  ->  Training Loss: 53663391989.08     Validation Loss: 75418349845.60\n",
      "Iteration  143  ->  Training Loss: 53591876074.52     Validation Loss: 75594845964.82\n",
      "Iteration  144  ->  Training Loss: 53521370829.43     Validation Loss: 75770859931.73\n",
      "Iteration  145  ->  Training Loss: 53451860237.92     Validation Loss: 75946380974.19\n",
      "Iteration  146  ->  Training Loss: 53383328539.70     Validation Loss: 76121398634.74\n",
      "Iteration  147  ->  Training Loss: 53315760226.00     Validation Loss: 76295902764.55\n",
      "Iteration  148  ->  Training Loss: 53249140035.56     Validation Loss: 76469883517.27\n",
      "Iteration  149  ->  Training Loss: 53183452950.64     Validation Loss: 76643331343.18\n",
      "Iteration  150  ->  Training Loss: 53118684193.18     Validation Loss: 76816236983.34\n",
      "Iteration  151  ->  Training Loss: 53054819220.94     Validation Loss: 76988591463.86\n",
      "Iteration  152  ->  Training Loss: 52991843723.75     Validation Loss: 77160386090.27\n",
      "Iteration  153  ->  Training Loss: 52929743619.80     Validation Loss: 77331612441.99\n",
      "Iteration  154  ->  Training Loss: 52868505051.98     Validation Loss: 77502262366.91\n",
      "Iteration  155  ->  Training Loss: 52808114384.31     Validation Loss: 77672327976.07\n",
      "Iteration  156  ->  Training Loss: 52748558198.40     Validation Loss: 77841801638.36\n",
      "Iteration  157  ->  Training Loss: 52689823289.97     Validation Loss: 78010675975.43\n",
      "Iteration  158  ->  Training Loss: 52631896665.45     Validation Loss: 78178943856.59\n",
      "Iteration  159  ->  Training Loss: 52574765538.59     Validation Loss: 78346598393.88\n",
      "Iteration  160  ->  Training Loss: 52518417327.16     Validation Loss: 78513632937.12\n",
      "Iteration  161  ->  Training Loss: 52462839649.69     Validation Loss: 78680041069.18\n",
      "Iteration  162  ->  Training Loss: 52408020322.28     Validation Loss: 78845816601.22\n",
      "Iteration  163  ->  Training Loss: 52353947355.43     Validation Loss: 79010953568.08\n",
      "Iteration  164  ->  Training Loss: 52300608950.91     Validation Loss: 79175446223.71\n",
      "Iteration  165  ->  Training Loss: 52247993498.76     Validation Loss: 79339289036.73\n",
      "Iteration  166  ->  Training Loss: 52196089574.24     Validation Loss: 79502476686.03\n",
      "Iteration  167  ->  Training Loss: 52144885934.90     Validation Loss: 79665004056.44\n",
      "Iteration  168  ->  Training Loss: 52094371517.64     Validation Loss: 79826866234.50\n",
      "Iteration  169  ->  Training Loss: 52044535435.88     Validation Loss: 79988058504.33\n",
      "Iteration  170  ->  Training Loss: 51995366976.70     Validation Loss: 80148576343.49\n",
      "Iteration  171  ->  Training Loss: 51946855598.11     Validation Loss: 80308415419.01\n",
      "Iteration  172  ->  Training Loss: 51898990926.29     Validation Loss: 80467571583.41\n",
      "Iteration  173  ->  Training Loss: 51851762752.92     Validation Loss: 80626040870.86\n",
      "Iteration  174  ->  Training Loss: 51805161032.51     Validation Loss: 80783819493.37\n",
      "Iteration  175  ->  Training Loss: 51759175879.85     Validation Loss: 80940903837.02\n",
      "Iteration  176  ->  Training Loss: 51713797567.42     Validation Loss: 81097290458.32\n",
      "Iteration  177  ->  Training Loss: 51669016522.84     Validation Loss: 81252976080.62\n",
      "Iteration  178  ->  Training Loss: 51624823326.45     Validation Loss: 81407957590.52\n",
      "Iteration  179  ->  Training Loss: 51581208708.86     Validation Loss: 81562232034.46\n",
      "Iteration  180  ->  Training Loss: 51538163548.50     Validation Loss: 81715796615.22\n",
      "Iteration  181  ->  Training Loss: 51495678869.34     Validation Loss: 81868648688.66\n",
      "Iteration  182  ->  Training Loss: 51453745838.47     Validation Loss: 82020785760.36\n",
      "Iteration  183  ->  Training Loss: 51412355763.92     Validation Loss: 82172205482.43\n",
      "Iteration  184  ->  Training Loss: 51371500092.32     Validation Loss: 82322905650.28\n",
      "Iteration  185  ->  Training Loss: 51331170406.73     Validation Loss: 82472884199.58\n",
      "Iteration  186  ->  Training Loss: 51291358424.47     Validation Loss: 82622139203.12\n",
      "Iteration  187  ->  Training Loss: 51252055994.93     Validation Loss: 82770668867.85\n",
      "Iteration  188  ->  Training Loss: 51213255097.52     Validation Loss: 82918471531.92\n",
      "Iteration  189  ->  Training Loss: 51174947839.56     Validation Loss: 83065545661.78\n",
      "Iteration  190  ->  Training Loss: 51137126454.21     Validation Loss: 83211889849.33\n",
      "Iteration  191  ->  Training Loss: 51099783298.54     Validation Loss: 83357502809.11\n",
      "Iteration  192  ->  Training Loss: 51062910851.46     Validation Loss: 83502383375.60\n",
      "Iteration  193  ->  Training Loss: 51026501711.86     Validation Loss: 83646530500.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  194  ->  Training Loss: 50990548596.61     Validation Loss: 83789943250.05\n",
      "Iteration  195  ->  Training Loss: 50955044338.75     Validation Loss: 83932620802.51\n",
      "Iteration  196  ->  Training Loss: 50919981885.59     Validation Loss: 84074562445.57\n",
      "Iteration  197  ->  Training Loss: 50885354296.90     Validation Loss: 84215767573.81\n",
      "Iteration  198  ->  Training Loss: 50851154743.11     Validation Loss: 84356235686.33\n",
      "Iteration  199  ->  Training Loss: 50817376503.55     Validation Loss: 84495966384.27\n",
      "Iteration  200  ->  Training Loss: 50784012964.71     Validation Loss: 84634959368.45\n",
      "Iteration  201  ->  Training Loss: 50751057618.50     Validation Loss: 84773214437.10\n",
      "Iteration  202  ->  Training Loss: 50718504060.63     Validation Loss: 84910731483.51\n",
      "Iteration  203  ->  Training Loss: 50686345988.88     Validation Loss: 85047510493.84\n",
      "Iteration  204  ->  Training Loss: 50654577201.54     Validation Loss: 85183551544.90\n",
      "Iteration  205  ->  Training Loss: 50623191595.73     Validation Loss: 85318854801.99\n",
      "Iteration  206  ->  Training Loss: 50592183165.89     Validation Loss: 85453420516.82\n",
      "Iteration  207  ->  Training Loss: 50561546002.19     Validation Loss: 85587249025.38\n",
      "Iteration  208  ->  Training Loss: 50531274289.00     Validation Loss: 85720340745.95\n",
      "Iteration  209  ->  Training Loss: 50501362303.40     Validation Loss: 85852696177.07\n",
      "Iteration  210  ->  Training Loss: 50471804413.71     Validation Loss: 85984315895.59\n",
      "Iteration  211  ->  Training Loss: 50442595077.99     Validation Loss: 86115200554.71\n",
      "Iteration  212  ->  Training Loss: 50413728842.64     Validation Loss: 86245350882.17\n",
      "Iteration  213  ->  Training Loss: 50385200341.01     Validation Loss: 86374767678.31\n",
      "Iteration  214  ->  Training Loss: 50357004291.96     Validation Loss: 86503451814.28\n",
      "Iteration  215  ->  Training Loss: 50329135498.52     Validation Loss: 86631404230.28\n",
      "Iteration  216  ->  Training Loss: 50301588846.56     Validation Loss: 86758625933.77\n",
      "Iteration  217  ->  Training Loss: 50274359303.44     Validation Loss: 86885117997.77\n",
      "Iteration  218  ->  Training Loss: 50247441916.73     Validation Loss: 87010881559.19\n",
      "Iteration  219  ->  Training Loss: 50220831812.92     Validation Loss: 87135917817.12\n",
      "Iteration  220  ->  Training Loss: 50194524196.15     Validation Loss: 87260228031.26\n",
      "Iteration  221  ->  Training Loss: 50168514347.01     Validation Loss: 87383813520.33\n",
      "Iteration  222  ->  Training Loss: 50142797621.25     Validation Loss: 87506675660.45\n",
      "Iteration  223  ->  Training Loss: 50117369448.66     Validation Loss: 87628815883.69\n",
      "Iteration  224  ->  Training Loss: 50092225331.83     Validation Loss: 87750235676.47\n",
      "Iteration  225  ->  Training Loss: 50067360845.00     Validation Loss: 87870936578.19\n",
      "Iteration  226  ->  Training Loss: 50042771632.94     Validation Loss: 87990920179.71\n",
      "Iteration  227  ->  Training Loss: 50018453409.78     Validation Loss: 88110188121.95\n",
      "Iteration  228  ->  Training Loss: 49994401957.95     Validation Loss: 88228742094.52\n",
      "Iteration  229  ->  Training Loss: 49970613127.06     Validation Loss: 88346583834.32\n",
      "Iteration  230  ->  Training Loss: 49947082832.85     Validation Loss: 88463715124.24\n",
      "Iteration  231  ->  Training Loss: 49923807056.08     Validation Loss: 88580137791.82\n",
      "Iteration  232  ->  Training Loss: 49900781841.59     Validation Loss: 88695853708.00\n",
      "Iteration  233  ->  Training Loss: 49878003297.15     Validation Loss: 88810864785.80\n",
      "Iteration  234  ->  Training Loss: 49855467592.59     Validation Loss: 88925172979.14\n",
      "Iteration  235  ->  Training Loss: 49833170958.70     Validation Loss: 89038780281.59\n",
      "Iteration  236  ->  Training Loss: 49811109686.30     Validation Loss: 89151688725.22\n",
      "Iteration  237  ->  Training Loss: 49789280125.31     Validation Loss: 89263900379.38\n",
      "Iteration  238  ->  Training Loss: 49767678683.75     Validation Loss: 89375417349.63\n",
      "Iteration  239  ->  Training Loss: 49746301826.85     Validation Loss: 89486241776.53\n",
      "Iteration  240  ->  Training Loss: 49725146076.13     Validation Loss: 89596375834.63\n",
      "Iteration  241  ->  Training Loss: 49704208008.48     Validation Loss: 89705821731.35\n",
      "Iteration  242  ->  Training Loss: 49683484255.32     Validation Loss: 89814581705.91\n",
      "Iteration  243  ->  Training Loss: 49662971501.68     Validation Loss: 89922658028.35\n",
      "Iteration  244  ->  Training Loss: 49642666485.39     Validation Loss: 90030052998.46\n",
      "Iteration  245  ->  Training Loss: 49622565996.18     Validation Loss: 90136768944.82\n",
      "Iteration  246  ->  Training Loss: 49602666874.93     Validation Loss: 90242808223.82\n",
      "Iteration  247  ->  Training Loss: 49582966012.77     Validation Loss: 90348173218.71\n",
      "Iteration  248  ->  Training Loss: 49563460350.33     Validation Loss: 90452866338.65\n",
      "Iteration  249  ->  Training Loss: 49544146876.95     Validation Loss: 90556890017.83\n",
      "Iteration  250  ->  Training Loss: 49525022629.87     Validation Loss: 90660246714.54\n",
      "Iteration  251  ->  Training Loss: 49506084693.50     Validation Loss: 90762938910.28\n",
      "Iteration  252  ->  Training Loss: 49487330198.63     Validation Loss: 90864969108.96\n",
      "Iteration  253  ->  Training Loss: 49468756321.73     Validation Loss: 90966339836.00\n",
      "Iteration  254  ->  Training Loss: 49450360284.20     Validation Loss: 91067053637.52\n",
      "Iteration  255  ->  Training Loss: 49432139351.65     Validation Loss: 91167113079.55\n",
      "Iteration  256  ->  Training Loss: 49414090833.18     Validation Loss: 91266520747.20\n",
      "Iteration  257  ->  Training Loss: 49396212080.75     Validation Loss: 91365279243.94\n",
      "Iteration  258  ->  Training Loss: 49378500488.40     Validation Loss: 91463391190.78\n",
      "Iteration  259  ->  Training Loss: 49360953491.66     Validation Loss: 91560859225.57\n",
      "Iteration  260  ->  Training Loss: 49343568566.84     Validation Loss: 91657686002.25\n",
      "Iteration  261  ->  Training Loss: 49326343230.41     Validation Loss: 91753874190.15\n",
      "Iteration  262  ->  Training Loss: 49309275038.32     Validation Loss: 91849426473.27\n",
      "Iteration  263  ->  Training Loss: 49292361585.43     Validation Loss: 91944345549.64\n",
      "Iteration  264  ->  Training Loss: 49275600504.82     Validation Loss: 92038634130.60\n",
      "Iteration  265  ->  Training Loss: 49258989467.22     Validation Loss: 92132294940.17\n",
      "Iteration  266  ->  Training Loss: 49242526180.41     Validation Loss: 92225330714.42\n",
      "Iteration  267  ->  Training Loss: 49226208388.61     Validation Loss: 92317744200.83\n",
      "Iteration  268  ->  Training Loss: 49210033871.92     Validation Loss: 92409538157.66\n",
      "Iteration  269  ->  Training Loss: 49194000445.73     Validation Loss: 92500715353.40\n",
      "Iteration  270  ->  Training Loss: 49178105960.16     Validation Loss: 92591278566.11\n",
      "Iteration  271  ->  Training Loss: 49162348299.50     Validation Loss: 92681230582.92\n",
      "Iteration  272  ->  Training Loss: 49146725381.70     Validation Loss: 92770574199.40\n",
      "Iteration  273  ->  Training Loss: 49131235157.77     Validation Loss: 92859312219.08\n",
      "Iteration  274  ->  Training Loss: 49115875611.31     Validation Loss: 92947447452.84\n",
      "Iteration  275  ->  Training Loss: 49100644757.97     Validation Loss: 93034982718.44\n",
      "Iteration  276  ->  Training Loss: 49085540644.92     Validation Loss: 93121920839.97\n",
      "Iteration  277  ->  Training Loss: 49070561350.38     Validation Loss: 93208264647.38\n",
      "Iteration  278  ->  Training Loss: 49055704983.09     Validation Loss: 93294016975.94\n",
      "Iteration  279  ->  Training Loss: 49040969681.84     Validation Loss: 93379180665.82\n",
      "Iteration  280  ->  Training Loss: 49026353615.02     Validation Loss: 93463758561.56\n",
      "Iteration  281  ->  Training Loss: 49011854980.08     Validation Loss: 93547753511.66\n",
      "Iteration  282  ->  Training Loss: 48997472003.13     Validation Loss: 93631168368.09\n",
      "Iteration  283  ->  Training Loss: 48983202938.45     Validation Loss: 93714005985.88\n",
      "Iteration  284  ->  Training Loss: 48969046068.03     Validation Loss: 93796269222.70\n",
      "Iteration  285  ->  Training Loss: 48954999701.17     Validation Loss: 93877960938.37\n",
      "Iteration  286  ->  Training Loss: 48941062174.02     Validation Loss: 93959083994.57\n",
      "Iteration  287  ->  Training Loss: 48927231849.12     Validation Loss: 94039641254.31\n",
      "Iteration  288  ->  Training Loss: 48913507115.03     Validation Loss: 94119635581.64\n",
      "Iteration  289  ->  Training Loss: 48899886385.91     Validation Loss: 94199069841.22\n",
      "Iteration  290  ->  Training Loss: 48886368101.07     Validation Loss: 94277946897.97\n",
      "Iteration  291  ->  Training Loss: 48872950724.60     Validation Loss: 94356269616.66\n",
      "Iteration  292  ->  Training Loss: 48859632744.98     Validation Loss: 94434040861.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  293  ->  Training Loss: 48846412674.67     Validation Loss: 94511263496.38\n",
      "Iteration  294  ->  Training Loss: 48833289049.73     Validation Loss: 94587940383.25\n",
      "Iteration  295  ->  Training Loss: 48820260429.46     Validation Loss: 94664074383.11\n",
      "Iteration  296  ->  Training Loss: 48807325396.03     Validation Loss: 94739668355.01\n",
      "Iteration  297  ->  Training Loss: 48794482554.08     Validation Loss: 94814725155.89\n",
      "Iteration  298  ->  Training Loss: 48781730530.41     Validation Loss: 94889247640.25\n",
      "Iteration  299  ->  Training Loss: 48769067973.59     Validation Loss: 94963238659.86\n",
      "Iteration  300  ->  Training Loss: 48756493553.61     Validation Loss: 95036701063.47\n",
      "Iteration  301  ->  Training Loss: 48744005961.59     Validation Loss: 95109637696.55\n",
      "Iteration  302  ->  Training Loss: 48731603909.38     Validation Loss: 95182051400.96\n",
      "Iteration  303  ->  Training Loss: 48719286129.26     Validation Loss: 95253945014.72\n",
      "Iteration  304  ->  Training Loss: 48707051373.60     Validation Loss: 95325321371.74\n",
      "Iteration  305  ->  Training Loss: 48694898414.57     Validation Loss: 95396183301.55\n",
      "Iteration  306  ->  Training Loss: 48682826043.79     Validation Loss: 95466533629.07\n",
      "Iteration  307  ->  Training Loss: 48670833072.03     Validation Loss: 95536375174.36\n",
      "Iteration  308  ->  Training Loss: 48658918328.91     Validation Loss: 95605710752.36\n",
      "Iteration  309  ->  Training Loss: 48647080662.60     Validation Loss: 95674543172.70\n",
      "Iteration  310  ->  Training Loss: 48635318939.52     Validation Loss: 95742875239.44\n",
      "Iteration  311  ->  Training Loss: 48623632044.05     Validation Loss: 95810709750.87\n",
      "Iteration  312  ->  Training Loss: 48612018878.23     Validation Loss: 95878049499.30\n",
      "Iteration  313  ->  Training Loss: 48600478361.50     Validation Loss: 95944897270.83\n",
      "Iteration  314  ->  Training Loss: 48589009430.41     Validation Loss: 96011255845.19\n",
      "Iteration  315  ->  Training Loss: 48577611038.36     Validation Loss: 96077127995.50\n",
      "Iteration  316  ->  Training Loss: 48566282155.32     Validation Loss: 96142516488.11\n",
      "Iteration  317  ->  Training Loss: 48555021767.55     Validation Loss: 96207424082.40\n",
      "Iteration  318  ->  Training Loss: 48543828877.37     Validation Loss: 96271853530.60\n",
      "Iteration  319  ->  Training Loss: 48532702502.89     Validation Loss: 96335807577.64\n",
      "Iteration  320  ->  Training Loss: 48521641677.77     Validation Loss: 96399288960.95\n",
      "Iteration  321  ->  Training Loss: 48510645450.94     Validation Loss: 96462300410.30\n",
      "Iteration  322  ->  Training Loss: 48499712886.38     Validation Loss: 96524844647.66\n",
      "Iteration  323  ->  Training Loss: 48488843062.90     Validation Loss: 96586924387.03\n",
      "Iteration  324  ->  Training Loss: 48478035073.86     Validation Loss: 96648542334.30\n",
      "Iteration  325  ->  Training Loss: 48467288026.96     Validation Loss: 96709701187.09\n",
      "Iteration  326  ->  Training Loss: 48456601044.01     Validation Loss: 96770403634.62\n",
      "Iteration  327  ->  Training Loss: 48445973260.73     Validation Loss: 96830652357.57\n",
      "Iteration  328  ->  Training Loss: 48435403826.47     Validation Loss: 96890450027.96\n",
      "Iteration  329  ->  Training Loss: 48424891904.03     Validation Loss: 96949799308.99\n",
      "Iteration  330  ->  Training Loss: 48414436669.47     Validation Loss: 97008702854.95\n",
      "Iteration  331  ->  Training Loss: 48404037311.83     Validation Loss: 97067163311.09\n",
      "Iteration  332  ->  Training Loss: 48393693032.99     Validation Loss: 97125183313.49\n",
      "Iteration  333  ->  Training Loss: 48383403047.44     Validation Loss: 97182765488.97\n",
      "Iteration  334  ->  Training Loss: 48373166582.05     Validation Loss: 97239912454.96\n",
      "Iteration  335  ->  Training Loss: 48362982875.92     Validation Loss: 97296626819.42\n",
      "Iteration  336  ->  Training Loss: 48352851180.18     Validation Loss: 97352911180.72\n",
      "Iteration  337  ->  Training Loss: 48342770757.74     Validation Loss: 97408768127.55\n",
      "Iteration  338  ->  Training Loss: 48332740883.18     Validation Loss: 97464200238.82\n",
      "Iteration  339  ->  Training Loss: 48322760842.51     Validation Loss: 97519210083.59\n",
      "Iteration  340  ->  Training Loss: 48312829933.02     Validation Loss: 97573800220.94\n",
      "Iteration  341  ->  Training Loss: 48302947463.08     Validation Loss: 97627973199.96\n",
      "Iteration  342  ->  Training Loss: 48293112751.97     Validation Loss: 97681731559.58\n",
      "Iteration  343  ->  Training Loss: 48283325129.71     Validation Loss: 97735077828.57\n",
      "Iteration  344  ->  Training Loss: 48273583936.86     Validation Loss: 97788014525.41\n",
      "Iteration  345  ->  Training Loss: 48263888524.42     Validation Loss: 97840544158.27\n",
      "Iteration  346  ->  Training Loss: 48254238253.59     Validation Loss: 97892669224.89\n",
      "Iteration  347  ->  Training Loss: 48244632495.64     Validation Loss: 97944392212.56\n",
      "Iteration  348  ->  Training Loss: 48235070631.75     Validation Loss: 97995715598.02\n",
      "Iteration  349  ->  Training Loss: 48225552052.86     Validation Loss: 98046641847.44\n",
      "Iteration  350  ->  Training Loss: 48216076159.49     Validation Loss: 98097173416.33\n",
      "Iteration  351  ->  Training Loss: 48206642361.60     Validation Loss: 98147312749.50\n",
      "Iteration  352  ->  Training Loss: 48197250078.46     Validation Loss: 98197062281.01\n",
      "Iteration  353  ->  Training Loss: 48187898738.45     Validation Loss: 98246424434.11\n",
      "Iteration  354  ->  Training Loss: 48178587778.97     Validation Loss: 98295401621.22\n",
      "Iteration  355  ->  Training Loss: 48169316646.26     Validation Loss: 98343996243.85\n",
      "Iteration  356  ->  Training Loss: 48160084795.30     Validation Loss: 98392210692.59\n",
      "Iteration  357  ->  Training Loss: 48150891689.59     Validation Loss: 98440047347.07\n",
      "Iteration  358  ->  Training Loss: 48141736801.12     Validation Loss: 98487508575.88\n",
      "Iteration  359  ->  Training Loss: 48132619610.16     Validation Loss: 98534596736.59\n",
      "Iteration  360  ->  Training Loss: 48123539605.13     Validation Loss: 98581314175.69\n",
      "Iteration  361  ->  Training Loss: 48114496282.53     Validation Loss: 98627663228.58\n",
      "Iteration  362  ->  Training Loss: 48105489146.73     Validation Loss: 98673646219.50\n",
      "Iteration  363  ->  Training Loss: 48096517709.92     Validation Loss: 98719265461.55\n",
      "Iteration  364  ->  Training Loss: 48087581491.93     Validation Loss: 98764523256.65\n",
      "Iteration  365  ->  Training Loss: 48078680020.14     Validation Loss: 98809421895.51\n",
      "Iteration  366  ->  Training Loss: 48069812829.33     Validation Loss: 98853963657.60\n",
      "Iteration  367  ->  Training Loss: 48060979461.60     Validation Loss: 98898150811.19\n",
      "Iteration  368  ->  Training Loss: 48052179466.24     Validation Loss: 98941985613.26\n",
      "Iteration  369  ->  Training Loss: 48043412399.59     Validation Loss: 98985470309.52\n",
      "Iteration  370  ->  Training Loss: 48034677824.96     Validation Loss: 99028607134.39\n",
      "Iteration  371  ->  Training Loss: 48025975312.49     Validation Loss: 99071398311.03\n",
      "Iteration  372  ->  Training Loss: 48017304439.09     Validation Loss: 99113846051.25\n",
      "Iteration  373  ->  Training Loss: 48008664788.26     Validation Loss: 99155952555.57\n",
      "Iteration  374  ->  Training Loss: 48000055950.06     Validation Loss: 99197720013.19\n",
      "Iteration  375  ->  Training Loss: 47991477520.95     Validation Loss: 99239150601.98\n",
      "Iteration  376  ->  Training Loss: 47982929103.70     Validation Loss: 99280246488.50\n",
      "Iteration  377  ->  Training Loss: 47974410307.33     Validation Loss: 99321009827.96\n",
      "Iteration  378  ->  Training Loss: 47965920746.95     Validation Loss: 99361442764.26\n",
      "Iteration  379  ->  Training Loss: 47957460043.70     Validation Loss: 99401547429.97\n",
      "Iteration  380  ->  Training Loss: 47949027824.65     Validation Loss: 99441325946.33\n",
      "Iteration  381  ->  Training Loss: 47940623722.69     Validation Loss: 99480780423.26\n",
      "Iteration  382  ->  Training Loss: 47932247376.46     Validation Loss: 99519912959.40\n",
      "Iteration  383  ->  Training Loss: 47923898430.23     Validation Loss: 99558725642.03\n",
      "Iteration  384  ->  Training Loss: 47915576533.85     Validation Loss: 99597220547.17\n",
      "Iteration  385  ->  Training Loss: 47907281342.62     Validation Loss: 99635399739.55\n",
      "Iteration  386  ->  Training Loss: 47899012517.22     Validation Loss: 99673265272.60\n",
      "Iteration  387  ->  Training Loss: 47890769723.65     Validation Loss: 99710819188.50\n",
      "Iteration  388  ->  Training Loss: 47882552633.08     Validation Loss: 99748063518.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  389  ->  Training Loss: 47874360921.86     Validation Loss: 99785000281.32\n",
      "Iteration  390  ->  Training Loss: 47866194271.33     Validation Loss: 99821631486.37\n",
      "Iteration  391  ->  Training Loss: 47858052367.84     Validation Loss: 99857959130.60\n",
      "Iteration  392  ->  Training Loss: 47849934902.61     Validation Loss: 99893985200.05\n",
      "Iteration  393  ->  Training Loss: 47841841571.67     Validation Loss: 99929711669.62\n",
      "Iteration  394  ->  Training Loss: 47833772075.79     Validation Loss: 99965140503.03\n",
      "Iteration  395  ->  Training Loss: 47825726120.37     Validation Loss: 100000273652.90\n",
      "Iteration  396  ->  Training Loss: 47817703415.44     Validation Loss: 100035113060.69\n",
      "Iteration  397  ->  Training Loss: 47809703675.49     Validation Loss: 100069660656.82\n",
      "Iteration  398  ->  Training Loss: 47801726619.50     Validation Loss: 100103918360.60\n",
      "Iteration  399  ->  Training Loss: 47793771970.77     Validation Loss: 100137888080.32\n",
      "Iteration  400  ->  Training Loss: 47785839456.93     Validation Loss: 100171571713.24\n",
      "Iteration  401  ->  Training Loss: 47777928809.84     Validation Loss: 100204971145.64\n",
      "Iteration  402  ->  Training Loss: 47770039765.49     Validation Loss: 100238088252.80\n",
      "Iteration  403  ->  Training Loss: 47762172064.00     Validation Loss: 100270924899.09\n",
      "Iteration  404  ->  Training Loss: 47754325449.52     Validation Loss: 100303482937.95\n",
      "Iteration  405  ->  Training Loss: 47746499670.15     Validation Loss: 100335764211.95\n",
      "Iteration  406  ->  Training Loss: 47738694477.89     Validation Loss: 100367770552.77\n",
      "Iteration  407  ->  Training Loss: 47730909628.62     Validation Loss: 100399503781.29\n",
      "Iteration  408  ->  Training Loss: 47723144881.96     Validation Loss: 100430965707.60\n",
      "Iteration  409  ->  Training Loss: 47715400001.27     Validation Loss: 100462158131.00\n",
      "Iteration  410  ->  Training Loss: 47707674753.57     Validation Loss: 100493082840.06\n",
      "Iteration  411  ->  Training Loss: 47699968909.47     Validation Loss: 100523741612.67\n",
      "Iteration  412  ->  Training Loss: 47692282243.15     Validation Loss: 100554136216.03\n",
      "Iteration  413  ->  Training Loss: 47684614532.27     Validation Loss: 100584268406.70\n",
      "Iteration  414  ->  Training Loss: 47676965557.90     Validation Loss: 100614139930.66\n",
      "Iteration  415  ->  Training Loss: 47669335104.54     Validation Loss: 100643752523.30\n",
      "Iteration  416  ->  Training Loss: 47661722959.96     Validation Loss: 100673107909.49\n",
      "Iteration  417  ->  Training Loss: 47654128915.25     Validation Loss: 100702207803.60\n",
      "Iteration  418  ->  Training Loss: 47646552764.70     Validation Loss: 100731053909.54\n",
      "Iteration  419  ->  Training Loss: 47638994305.75     Validation Loss: 100759647920.79\n",
      "Iteration  420  ->  Training Loss: 47631453339.01     Validation Loss: 100787991520.45\n",
      "Iteration  421  ->  Training Loss: 47623929668.10     Validation Loss: 100816086381.25\n",
      "Iteration  422  ->  Training Loss: 47616423099.72     Validation Loss: 100843934165.64\n",
      "Iteration  423  ->  Training Loss: 47608933443.49     Validation Loss: 100871536525.77\n",
      "Iteration  424  ->  Training Loss: 47601460512.00     Validation Loss: 100898895103.56\n",
      "Iteration  425  ->  Training Loss: 47594004120.69     Validation Loss: 100926011530.73\n",
      "Iteration  426  ->  Training Loss: 47586564087.84     Validation Loss: 100952887428.85\n",
      "Iteration  427  ->  Training Loss: 47579140234.53     Validation Loss: 100979524409.35\n",
      "Iteration  428  ->  Training Loss: 47571732384.57     Validation Loss: 101005924073.62\n",
      "Iteration  429  ->  Training Loss: 47564340364.47     Validation Loss: 101032088012.96\n",
      "Iteration  430  ->  Training Loss: 47556964003.43     Validation Loss: 101058017808.72\n",
      "Iteration  431  ->  Training Loss: 47549603133.21     Validation Loss: 101083715032.27\n",
      "Iteration  432  ->  Training Loss: 47542257588.20     Validation Loss: 101109181245.06\n",
      "Iteration  433  ->  Training Loss: 47534927205.28     Validation Loss: 101134417998.67\n",
      "Iteration  434  ->  Training Loss: 47527611823.84     Validation Loss: 101159426834.86\n",
      "Iteration  435  ->  Training Loss: 47520311285.74     Validation Loss: 101184209285.57\n",
      "Iteration  436  ->  Training Loss: 47513025435.22     Validation Loss: 101208766873.03\n",
      "Iteration  437  ->  Training Loss: 47505754118.93     Validation Loss: 101233101109.73\n",
      "Iteration  438  ->  Training Loss: 47498497185.84     Validation Loss: 101257213498.50\n",
      "Iteration  439  ->  Training Loss: 47491254487.21     Validation Loss: 101281105532.58\n",
      "Iteration  440  ->  Training Loss: 47484025876.60     Validation Loss: 101304778695.58\n",
      "Iteration  441  ->  Training Loss: 47476811209.76     Validation Loss: 101328234461.63\n",
      "Iteration  442  ->  Training Loss: 47469610344.66     Validation Loss: 101351474295.33\n",
      "Iteration  443  ->  Training Loss: 47462423141.42     Validation Loss: 101374499651.85\n",
      "Iteration  444  ->  Training Loss: 47455249462.28     Validation Loss: 101397311976.94\n",
      "Iteration  445  ->  Training Loss: 47448089171.59     Validation Loss: 101419912707.01\n",
      "Iteration  446  ->  Training Loss: 47440942135.72     Validation Loss: 101442303269.15\n",
      "Iteration  447  ->  Training Loss: 47433808223.11     Validation Loss: 101464485081.15\n",
      "Iteration  448  ->  Training Loss: 47426687304.15     Validation Loss: 101486459551.61\n",
      "Iteration  449  ->  Training Loss: 47419579251.23     Validation Loss: 101508228079.93\n",
      "Iteration  450  ->  Training Loss: 47412483938.65     Validation Loss: 101529792056.36\n",
      "Iteration  451  ->  Training Loss: 47405401242.60     Validation Loss: 101551152862.07\n",
      "Iteration  452  ->  Training Loss: 47398331041.15     Validation Loss: 101572311869.18\n",
      "Iteration  453  ->  Training Loss: 47391273214.23     Validation Loss: 101593270440.79\n",
      "Iteration  454  ->  Training Loss: 47384227643.55     Validation Loss: 101614029931.06\n",
      "Iteration  455  ->  Training Loss: 47377194212.61     Validation Loss: 101634591685.22\n",
      "Iteration  456  ->  Training Loss: 47370172806.67     Validation Loss: 101654957039.63\n",
      "Iteration  457  ->  Training Loss: 47363163312.71     Validation Loss: 101675127321.84\n",
      "Iteration  458  ->  Training Loss: 47356165619.43     Validation Loss: 101695103850.59\n",
      "Iteration  459  ->  Training Loss: 47349179617.16     Validation Loss: 101714887935.91\n",
      "Iteration  460  ->  Training Loss: 47342205197.91     Validation Loss: 101734480879.13\n",
      "Iteration  461  ->  Training Loss: 47335242255.30     Validation Loss: 101753883972.94\n",
      "Iteration  462  ->  Training Loss: 47328290684.54     Validation Loss: 101773098501.41\n",
      "Iteration  463  ->  Training Loss: 47321350382.41     Validation Loss: 101792125740.08\n",
      "Iteration  464  ->  Training Loss: 47314421247.23     Validation Loss: 101810966955.97\n",
      "Iteration  465  ->  Training Loss: 47307503178.84     Validation Loss: 101829623407.64\n",
      "Iteration  466  ->  Training Loss: 47300596078.58     Validation Loss: 101848096345.21\n",
      "Iteration  467  ->  Training Loss: 47293699849.27     Validation Loss: 101866387010.45\n",
      "Iteration  468  ->  Training Loss: 47286814395.16     Validation Loss: 101884496636.79\n",
      "Iteration  469  ->  Training Loss: 47279939621.92     Validation Loss: 101902426449.38\n",
      "Iteration  470  ->  Training Loss: 47273075436.65     Validation Loss: 101920177665.12\n",
      "Iteration  471  ->  Training Loss: 47266221747.81     Validation Loss: 101937751492.72\n",
      "Iteration  472  ->  Training Loss: 47259378465.22     Validation Loss: 101955149132.76\n",
      "Iteration  473  ->  Training Loss: 47252545500.05     Validation Loss: 101972371777.69\n",
      "Iteration  474  ->  Training Loss: 47245722764.77     Validation Loss: 101989420611.91\n",
      "Iteration  475  ->  Training Loss: 47238910173.15     Validation Loss: 102006296811.80\n",
      "Iteration  476  ->  Training Loss: 47232107640.25     Validation Loss: 102023001545.79\n",
      "Iteration  477  ->  Training Loss: 47225315082.35     Validation Loss: 102039535974.37\n",
      "Iteration  478  ->  Training Loss: 47218532417.01     Validation Loss: 102055901250.14\n",
      "Iteration  479  ->  Training Loss: 47211759562.96     Validation Loss: 102072098517.89\n",
      "Iteration  480  ->  Training Loss: 47204996440.16     Validation Loss: 102088128914.59\n",
      "Iteration  481  ->  Training Loss: 47198242969.74     Validation Loss: 102103993569.49\n",
      "Iteration  482  ->  Training Loss: 47191499073.97     Validation Loss: 102119693604.13\n",
      "Iteration  483  ->  Training Loss: 47184764676.28     Validation Loss: 102135230132.38\n",
      "Iteration  484  ->  Training Loss: 47178039701.21     Validation Loss: 102150604260.50\n",
      "Iteration  485  ->  Training Loss: 47171324074.42     Validation Loss: 102165817087.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  486  ->  Training Loss: 47164617722.65     Validation Loss: 102180869703.64\n",
      "Iteration  487  ->  Training Loss: 47157920573.69     Validation Loss: 102195763193.52\n",
      "Iteration  488  ->  Training Loss: 47151232556.42     Validation Loss: 102210498633.09\n",
      "Iteration  489  ->  Training Loss: 47144553600.72     Validation Loss: 102225077091.20\n",
      "Iteration  490  ->  Training Loss: 47137883637.52     Validation Loss: 102239499629.36\n",
      "Iteration  491  ->  Training Loss: 47131222598.74     Validation Loss: 102253767301.77\n",
      "Iteration  492  ->  Training Loss: 47124570417.29     Validation Loss: 102267881155.38\n",
      "Iteration  493  ->  Training Loss: 47117927027.04     Validation Loss: 102281842229.89\n",
      "Iteration  494  ->  Training Loss: 47111292362.85     Validation Loss: 102295651557.86\n",
      "Iteration  495  ->  Training Loss: 47104666360.48     Validation Loss: 102309310164.69\n",
      "Iteration  496  ->  Training Loss: 47098048956.66     Validation Loss: 102322819068.70\n",
      "Iteration  497  ->  Training Loss: 47091440089.00     Validation Loss: 102336179281.15\n",
      "Iteration  498  ->  Training Loss: 47084839696.02     Validation Loss: 102349391806.32\n",
      "Iteration  499  ->  Training Loss: 47078247717.13     Validation Loss: 102362457641.50\n",
      "Iteration  500  ->  Training Loss: 47071664092.61     Validation Loss: 102375377777.08\n",
      "Iteration  501  ->  Training Loss: 47065088763.58     Validation Loss: 102388153196.55\n",
      "Iteration  502  ->  Training Loss: 47058521672.02     Validation Loss: 102400784876.60\n",
      "Iteration  503  ->  Training Loss: 47051962760.73     Validation Loss: 102413273787.09\n",
      "Iteration  504  ->  Training Loss: 47045411973.34     Validation Loss: 102425620891.15\n",
      "Iteration  505  ->  Training Loss: 47038869254.28     Validation Loss: 102437827145.19\n",
      "Iteration  506  ->  Training Loss: 47032334548.76     Validation Loss: 102449893498.95\n",
      "Iteration  507  ->  Training Loss: 47025807802.77     Validation Loss: 102461820895.55\n",
      "Iteration  508  ->  Training Loss: 47019288963.08     Validation Loss: 102473610271.53\n",
      "Iteration  509  ->  Training Loss: 47012777977.21     Validation Loss: 102485262556.86\n",
      "Iteration  510  ->  Training Loss: 47006274793.42     Validation Loss: 102496778675.04\n",
      "Iteration  511  ->  Training Loss: 46999779360.69     Validation Loss: 102508159543.09\n",
      "Iteration  512  ->  Training Loss: 46993291628.75     Validation Loss: 102519406071.59\n",
      "Iteration  513  ->  Training Loss: 46986811548.01     Validation Loss: 102530519164.78\n",
      "Iteration  514  ->  Training Loss: 46980339069.58     Validation Loss: 102541499720.52\n",
      "Iteration  515  ->  Training Loss: 46973874145.27     Validation Loss: 102552348630.39\n",
      "Iteration  516  ->  Training Loss: 46967416727.57     Validation Loss: 102563066779.71\n",
      "Iteration  517  ->  Training Loss: 46960966769.60     Validation Loss: 102573655047.57\n",
      "Iteration  518  ->  Training Loss: 46954524225.17     Validation Loss: 102584114306.87\n",
      "Iteration  519  ->  Training Loss: 46948089048.73     Validation Loss: 102594445424.40\n",
      "Iteration  520  ->  Training Loss: 46941661195.34     Validation Loss: 102604649260.81\n",
      "Iteration  521  ->  Training Loss: 46935240620.72     Validation Loss: 102614726670.71\n",
      "Iteration  522  ->  Training Loss: 46928827281.17     Validation Loss: 102624678502.69\n",
      "Iteration  523  ->  Training Loss: 46922421133.61     Validation Loss: 102634505599.33\n",
      "Iteration  524  ->  Training Loss: 46916022135.56     Validation Loss: 102644208797.29\n",
      "Iteration  525  ->  Training Loss: 46909630245.13     Validation Loss: 102653788927.30\n",
      "Iteration  526  ->  Training Loss: 46903245420.98     Validation Loss: 102663246814.24\n",
      "Iteration  527  ->  Training Loss: 46896867622.38     Validation Loss: 102672583277.15\n",
      "Iteration  528  ->  Training Loss: 46890496809.11     Validation Loss: 102681799129.27\n",
      "Iteration  529  ->  Training Loss: 46884132941.54     Validation Loss: 102690895178.10\n",
      "Iteration  530  ->  Training Loss: 46877775980.56     Validation Loss: 102699872225.41\n",
      "Iteration  531  ->  Training Loss: 46871425887.60     Validation Loss: 102708731067.29\n",
      "Iteration  532  ->  Training Loss: 46865082624.63     Validation Loss: 102717472494.20\n",
      "Iteration  533  ->  Training Loss: 46858746154.10     Validation Loss: 102726097290.98\n",
      "Iteration  534  ->  Training Loss: 46852416439.01     Validation Loss: 102734606236.90\n",
      "Iteration  535  ->  Training Loss: 46846093442.82     Validation Loss: 102743000105.71\n",
      "Iteration  536  ->  Training Loss: 46839777129.53     Validation Loss: 102751279665.66\n",
      "Iteration  537  ->  Training Loss: 46833467463.57     Validation Loss: 102759445679.54\n",
      "Iteration  538  ->  Training Loss: 46827164409.90     Validation Loss: 102767498904.72\n",
      "Iteration  539  ->  Training Loss: 46820867933.91     Validation Loss: 102775440093.18\n",
      "Iteration  540  ->  Training Loss: 46814578001.48     Validation Loss: 102783269991.55\n",
      "Iteration  541  ->  Training Loss: 46808294578.93     Validation Loss: 102790989341.15\n",
      "Iteration  542  ->  Training Loss: 46802017633.04     Validation Loss: 102798598878.01\n",
      "Iteration  543  ->  Training Loss: 46795747131.01     Validation Loss: 102806099332.94\n",
      "Iteration  544  ->  Training Loss: 46789483040.51     Validation Loss: 102813491431.50\n",
      "Iteration  545  ->  Training Loss: 46783225329.60     Validation Loss: 102820775894.13\n",
      "Iteration  546  ->  Training Loss: 46776973966.80     Validation Loss: 102827953436.08\n",
      "Iteration  547  ->  Training Loss: 46770728921.00     Validation Loss: 102835024767.54\n",
      "Iteration  548  ->  Training Loss: 46764490161.53     Validation Loss: 102841990593.59\n",
      "Iteration  549  ->  Training Loss: 46758257658.12     Validation Loss: 102848851614.30\n",
      "Iteration  550  ->  Training Loss: 46752031380.89     Validation Loss: 102855608524.74\n",
      "Iteration  551  ->  Training Loss: 46745811300.34     Validation Loss: 102862262015.00\n",
      "Iteration  552  ->  Training Loss: 46739597387.37     Validation Loss: 102868812770.25\n",
      "Iteration  553  ->  Training Loss: 46733389613.25     Validation Loss: 102875261470.75\n",
      "Iteration  554  ->  Training Loss: 46727187949.62     Validation Loss: 102881608791.90\n",
      "Iteration  555  ->  Training Loss: 46720992368.48     Validation Loss: 102887855404.25\n",
      "Iteration  556  ->  Training Loss: 46714802842.22     Validation Loss: 102894001973.58\n",
      "Iteration  557  ->  Training Loss: 46708619343.54     Validation Loss: 102900049160.88\n",
      "Iteration  558  ->  Training Loss: 46702441845.52     Validation Loss: 102905997622.42\n",
      "Iteration  559  ->  Training Loss: 46696270321.58     Validation Loss: 102911848009.74\n",
      "Iteration  560  ->  Training Loss: 46690104745.46     Validation Loss: 102917600969.75\n",
      "Iteration  561  ->  Training Loss: 46683945091.27     Validation Loss: 102923257144.70\n",
      "Iteration  562  ->  Training Loss: 46677791333.41     Validation Loss: 102928817172.23\n",
      "Iteration  563  ->  Training Loss: 46671643446.63     Validation Loss: 102934281685.42\n",
      "Iteration  564  ->  Training Loss: 46665501405.98     Validation Loss: 102939651312.80\n",
      "Iteration  565  ->  Training Loss: 46659365186.82     Validation Loss: 102944926678.39\n",
      "Iteration  566  ->  Training Loss: 46653234764.85     Validation Loss: 102950108401.73\n",
      "Iteration  567  ->  Training Loss: 46647110116.03     Validation Loss: 102955197097.91\n",
      "Iteration  568  ->  Training Loss: 46640991216.66     Validation Loss: 102960193377.62\n",
      "Iteration  569  ->  Training Loss: 46634878043.30     Validation Loss: 102965097847.13\n",
      "Iteration  570  ->  Training Loss: 46628770572.83     Validation Loss: 102969911108.38\n",
      "Iteration  571  ->  Training Loss: 46622668782.39     Validation Loss: 102974633758.99\n",
      "Iteration  572  ->  Training Loss: 46616572649.41     Validation Loss: 102979266392.25\n",
      "Iteration  573  ->  Training Loss: 46610482151.60     Validation Loss: 102983809597.21\n",
      "Iteration  574  ->  Training Loss: 46604397266.94     Validation Loss: 102988263958.69\n",
      "Iteration  575  ->  Training Loss: 46598317973.68     Validation Loss: 102992630057.29\n",
      "Iteration  576  ->  Training Loss: 46592244250.34     Validation Loss: 102996908469.43\n",
      "Iteration  577  ->  Training Loss: 46586176075.68     Validation Loss: 103001099767.39\n",
      "Iteration  578  ->  Training Loss: 46580113428.73     Validation Loss: 103005204519.34\n",
      "Iteration  579  ->  Training Loss: 46574056288.77     Validation Loss: 103009223289.34\n",
      "Iteration  580  ->  Training Loss: 46568004635.34     Validation Loss: 103013156637.41\n",
      "Iteration  581  ->  Training Loss: 46561958448.20     Validation Loss: 103017005119.52\n",
      "Iteration  582  ->  Training Loss: 46555917707.37     Validation Loss: 103020769287.66\n",
      "Iteration  583  ->  Training Loss: 46549882393.11     Validation Loss: 103024449689.83\n",
      "Iteration  584  ->  Training Loss: 46543852485.88     Validation Loss: 103028046870.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  585  ->  Training Loss: 46537827966.42     Validation Loss: 103031561368.53\n",
      "Iteration  586  ->  Training Loss: 46531808815.66     Validation Loss: 103034993721.44\n",
      "Iteration  587  ->  Training Loss: 46525795014.76     Validation Loss: 103038344461.19\n",
      "Iteration  588  ->  Training Loss: 46519786545.11     Validation Loss: 103041614116.32\n",
      "Iteration  589  ->  Training Loss: 46513783388.30     Validation Loss: 103044803211.57\n",
      "Iteration  590  ->  Training Loss: 46507785526.15     Validation Loss: 103047912267.88\n",
      "Iteration  591  ->  Training Loss: 46501792940.68     Validation Loss: 103050941802.46\n",
      "Iteration  592  ->  Training Loss: 46495805614.12     Validation Loss: 103053892328.78\n",
      "Iteration  593  ->  Training Loss: 46489823528.90     Validation Loss: 103056764356.59\n",
      "Iteration  594  ->  Training Loss: 46483846667.64     Validation Loss: 103059558392.01\n",
      "Iteration  595  ->  Training Loss: 46477875013.19     Validation Loss: 103062274937.46\n",
      "Iteration  596  ->  Training Loss: 46471908548.56     Validation Loss: 103064914491.78\n",
      "Iteration  597  ->  Training Loss: 46465947256.97     Validation Loss: 103067477550.18\n",
      "Iteration  598  ->  Training Loss: 46459991121.82     Validation Loss: 103069964604.31\n",
      "Iteration  599  ->  Training Loss: 46454040126.70     Validation Loss: 103072376142.29\n",
      "Iteration  600  ->  Training Loss: 46448094255.37     Validation Loss: 103074712648.70\n",
      "Iteration  601  ->  Training Loss: 46442153491.80     Validation Loss: 103076974604.63\n",
      "Iteration  602  ->  Training Loss: 46436217820.10     Validation Loss: 103079162487.71\n",
      "Iteration  603  ->  Training Loss: 46430287224.59     Validation Loss: 103081276772.10\n",
      "Iteration  604  ->  Training Loss: 46424361689.73     Validation Loss: 103083317928.58\n",
      "Iteration  605  ->  Training Loss: 46418441200.17     Validation Loss: 103085286424.49\n",
      "Iteration  606  ->  Training Loss: 46412525740.72     Validation Loss: 103087182723.84\n",
      "Iteration  607  ->  Training Loss: 46406615296.35     Validation Loss: 103089007287.27\n",
      "Iteration  608  ->  Training Loss: 46400709852.21     Validation Loss: 103090760572.09\n",
      "Iteration  609  ->  Training Loss: 46394809393.58     Validation Loss: 103092443032.34\n",
      "Iteration  610  ->  Training Loss: 46388913905.93     Validation Loss: 103094055118.75\n",
      "Iteration  611  ->  Training Loss: 46383023374.85     Validation Loss: 103095597278.84\n",
      "Iteration  612  ->  Training Loss: 46377137786.12     Validation Loss: 103097069956.87\n",
      "Iteration  613  ->  Training Loss: 46371257125.63     Validation Loss: 103098473593.91\n",
      "Iteration  614  ->  Training Loss: 46365381379.45     Validation Loss: 103099808627.84\n",
      "Iteration  615  ->  Training Loss: 46359510533.79     Validation Loss: 103101075493.40\n",
      "Iteration  616  ->  Training Loss: 46353644574.98     Validation Loss: 103102274622.18\n",
      "Iteration  617  ->  Training Loss: 46347783489.52     Validation Loss: 103103406442.65\n",
      "Iteration  618  ->  Training Loss: 46341927264.03     Validation Loss: 103104471380.23\n",
      "Iteration  619  ->  Training Loss: 46336075885.28     Validation Loss: 103105469857.23\n",
      "Iteration  620  ->  Training Loss: 46330229340.17     Validation Loss: 103106402292.93\n",
      "Iteration  621  ->  Training Loss: 46324387615.73     Validation Loss: 103107269103.59\n",
      "Iteration  622  ->  Training Loss: 46318550699.13     Validation Loss: 103108070702.48\n",
      "Iteration  623  ->  Training Loss: 46312718577.65     Validation Loss: 103108807499.88\n",
      "Iteration  624  ->  Training Loss: 46306891238.73     Validation Loss: 103109479903.11\n",
      "Iteration  625  ->  Training Loss: 46301068669.90     Validation Loss: 103110088316.56\n",
      "Iteration  626  ->  Training Loss: 46295250858.83     Validation Loss: 103110633141.72\n",
      "Iteration  627  ->  Training Loss: 46289437793.31     Validation Loss: 103111114777.16\n",
      "Iteration  628  ->  Training Loss: 46283629461.27     Validation Loss: 103111533618.59\n",
      "Iteration  629  ->  Training Loss: 46277825850.71     Validation Loss: 103111890058.90\n",
      "Iteration  630  ->  Training Loss: 46272026949.79     Validation Loss: 103112184488.10\n",
      "Iteration  631  ->  Training Loss: 46266232746.78     Validation Loss: 103112417293.42\n",
      "Iteration  632  ->  Training Loss: 46260443230.03     Validation Loss: 103112588859.31\n",
      "Iteration  633  ->  Training Loss: 46254658388.04     Validation Loss: 103112699567.44\n",
      "Iteration  634  ->  Training Loss: 46248878209.40     Validation Loss: 103112749796.73\n",
      "Iteration  635  ->  Training Loss: 46243102682.81     Validation Loss: 103112739923.38\n",
      "Iteration  636  ->  Training Loss: 46237331797.09     Validation Loss: 103112670320.89\n",
      "Iteration  637  ->  Training Loss: 46231565541.13     Validation Loss: 103112541360.05\n",
      "Iteration  638  ->  Training Loss: 46225803903.97     Validation Loss: 103112353409.01\n",
      "Iteration  639  ->  Training Loss: 46220046874.71     Validation Loss: 103112106833.26\n",
      "Iteration  640  ->  Training Loss: 46214294442.58     Validation Loss: 103111801995.66\n",
      "Iteration  641  ->  Training Loss: 46208546596.90     Validation Loss: 103111439256.47\n",
      "Iteration  642  ->  Training Loss: 46202803327.08     Validation Loss: 103111018973.35\n",
      "Iteration  643  ->  Training Loss: 46197064622.64     Validation Loss: 103110541501.40\n",
      "Iteration  644  ->  Training Loss: 46191330473.17     Validation Loss: 103110007193.17\n",
      "Iteration  645  ->  Training Loss: 46185600868.38     Validation Loss: 103109416398.66\n",
      "Iteration  646  ->  Training Loss: 46179875798.07     Validation Loss: 103108769465.38\n",
      "Iteration  647  ->  Training Loss: 46174155252.11     Validation Loss: 103108066738.33\n",
      "Iteration  648  ->  Training Loss: 46168439220.48     Validation Loss: 103107308560.04\n",
      "Iteration  649  ->  Training Loss: 46162727693.23     Validation Loss: 103106495270.58\n",
      "Iteration  650  ->  Training Loss: 46157020660.51     Validation Loss: 103105627207.58\n",
      "Iteration  651  ->  Training Loss: 46151318112.56     Validation Loss: 103104704706.25\n",
      "Iteration  652  ->  Training Loss: 46145620039.69     Validation Loss: 103103728099.39\n",
      "Iteration  653  ->  Training Loss: 46139926432.31     Validation Loss: 103102697717.42\n",
      "Iteration  654  ->  Training Loss: 46134237280.88     Validation Loss: 103101613888.40\n",
      "Iteration  655  ->  Training Loss: 46128552575.98     Validation Loss: 103100476938.02\n",
      "Iteration  656  ->  Training Loss: 46122872308.25     Validation Loss: 103099287189.66\n",
      "Iteration  657  ->  Training Loss: 46117196468.41     Validation Loss: 103098044964.35\n",
      "Iteration  658  ->  Training Loss: 46111525047.25     Validation Loss: 103096750580.86\n",
      "Iteration  659  ->  Training Loss: 46105858035.66     Validation Loss: 103095404355.65\n",
      "Iteration  660  ->  Training Loss: 46100195424.57     Validation Loss: 103094006602.93\n",
      "Iteration  661  ->  Training Loss: 46094537205.02     Validation Loss: 103092557634.66\n",
      "Iteration  662  ->  Training Loss: 46088883368.10     Validation Loss: 103091057760.57\n",
      "Iteration  663  ->  Training Loss: 46083233904.98     Validation Loss: 103089507288.16\n",
      "Iteration  664  ->  Training Loss: 46077588806.89     Validation Loss: 103087906522.76\n",
      "Iteration  665  ->  Training Loss: 46071948065.16     Validation Loss: 103086255767.51\n",
      "Iteration  666  ->  Training Loss: 46066311671.15     Validation Loss: 103084555323.35\n",
      "Iteration  667  ->  Training Loss: 46060679616.32     Validation Loss: 103082805489.13\n",
      "Iteration  668  ->  Training Loss: 46055051892.17     Validation Loss: 103081006561.52\n",
      "Iteration  669  ->  Training Loss: 46049428490.30     Validation Loss: 103079158835.09\n",
      "Iteration  670  ->  Training Loss: 46043809402.33     Validation Loss: 103077262602.32\n",
      "Iteration  671  ->  Training Loss: 46038194620.00     Validation Loss: 103075318153.58\n",
      "Iteration  672  ->  Training Loss: 46032584135.06     Validation Loss: 103073325777.18\n",
      "Iteration  673  ->  Training Loss: 46026977939.35     Validation Loss: 103071285759.40\n",
      "Iteration  674  ->  Training Loss: 46021376024.77     Validation Loss: 103069198384.43\n",
      "Iteration  675  ->  Training Loss: 46015778383.29     Validation Loss: 103067063934.49\n",
      "Iteration  676  ->  Training Loss: 46010185006.91     Validation Loss: 103064882689.76\n",
      "Iteration  677  ->  Training Loss: 46004595887.71     Validation Loss: 103062654928.42\n",
      "Iteration  678  ->  Training Loss: 45999011017.84     Validation Loss: 103060380926.70\n",
      "Iteration  679  ->  Training Loss: 45993430389.49     Validation Loss: 103058060958.84\n",
      "Iteration  680  ->  Training Loss: 45987853994.90     Validation Loss: 103055695297.15\n",
      "Iteration  681  ->  Training Loss: 45982281826.38     Validation Loss: 103053284211.99\n",
      "Iteration  682  ->  Training Loss: 45976713876.30     Validation Loss: 103050827971.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  683  ->  Training Loss: 45971150137.06     Validation Loss: 103048326843.14\n",
      "Iteration  684  ->  Training Loss: 45965590601.15     Validation Loss: 103045781090.64\n",
      "Iteration  685  ->  Training Loss: 45960035261.08     Validation Loss: 103043190977.09\n",
      "Iteration  686  ->  Training Loss: 45954484109.43     Validation Loss: 103040556763.39\n",
      "Iteration  687  ->  Training Loss: 45948937138.83     Validation Loss: 103037878708.59\n",
      "Iteration  688  ->  Training Loss: 45943394341.95     Validation Loss: 103035157069.94\n",
      "Iteration  689  ->  Training Loss: 45937855711.52     Validation Loss: 103032392102.82\n",
      "Iteration  690  ->  Training Loss: 45932321240.32     Validation Loss: 103029584060.83\n",
      "Iteration  691  ->  Training Loss: 45926790921.18     Validation Loss: 103026733195.78\n",
      "Iteration  692  ->  Training Loss: 45921264746.97     Validation Loss: 103023839757.69\n",
      "Iteration  693  ->  Training Loss: 45915742710.61     Validation Loss: 103020903994.81\n",
      "Iteration  694  ->  Training Loss: 45910224805.07     Validation Loss: 103017926153.64\n",
      "Iteration  695  ->  Training Loss: 45904711023.37     Validation Loss: 103014906478.95\n",
      "Iteration  696  ->  Training Loss: 45899201358.57     Validation Loss: 103011845213.75\n",
      "Iteration  697  ->  Training Loss: 45893695803.78     Validation Loss: 103008742599.37\n",
      "Iteration  698  ->  Training Loss: 45888194352.14     Validation Loss: 103005598875.43\n",
      "Iteration  699  ->  Training Loss: 45882696996.85     Validation Loss: 103002414279.85\n",
      "Iteration  700  ->  Training Loss: 45877203731.15     Validation Loss: 102999189048.88\n",
      "Iteration  701  ->  Training Loss: 45871714548.32     Validation Loss: 102995923417.11\n",
      "Iteration  702  ->  Training Loss: 45866229441.68     Validation Loss: 102992617617.47\n",
      "Iteration  703  ->  Training Loss: 45860748404.61     Validation Loss: 102989271881.27\n",
      "Iteration  704  ->  Training Loss: 45855271430.49     Validation Loss: 102985886438.16\n",
      "Iteration  705  ->  Training Loss: 45849798512.79     Validation Loss: 102982461516.22\n",
      "Iteration  706  ->  Training Loss: 45844329644.99     Validation Loss: 102978997341.89\n",
      "Iteration  707  ->  Training Loss: 45838864820.60     Validation Loss: 102975494140.04\n",
      "Iteration  708  ->  Training Loss: 45833404033.21     Validation Loss: 102971952133.96\n",
      "Iteration  709  ->  Training Loss: 45827947276.41     Validation Loss: 102968371545.38\n",
      "Iteration  710  ->  Training Loss: 45822494543.84     Validation Loss: 102964752594.45\n",
      "Iteration  711  ->  Training Loss: 45817045829.19     Validation Loss: 102961095499.81\n",
      "Iteration  712  ->  Training Loss: 45811601126.16     Validation Loss: 102957400478.56\n",
      "Iteration  713  ->  Training Loss: 45806160428.52     Validation Loss: 102953667746.27\n",
      "Iteration  714  ->  Training Loss: 45800723730.05     Validation Loss: 102949897517.00\n",
      "Iteration  715  ->  Training Loss: 45795291024.58     Validation Loss: 102946090003.34\n",
      "Iteration  716  ->  Training Loss: 45789862305.97     Validation Loss: 102942245416.38\n",
      "Iteration  717  ->  Training Loss: 45784437568.10     Validation Loss: 102938363965.72\n",
      "Iteration  718  ->  Training Loss: 45779016804.92     Validation Loss: 102934445859.52\n",
      "Iteration  719  ->  Training Loss: 45773600010.38     Validation Loss: 102930491304.48\n",
      "Iteration  720  ->  Training Loss: 45768187178.48     Validation Loss: 102926500505.87\n",
      "Iteration  721  ->  Training Loss: 45762778303.25     Validation Loss: 102922473667.52\n",
      "Iteration  722  ->  Training Loss: 45757373378.76     Validation Loss: 102918410991.83\n",
      "Iteration  723  ->  Training Loss: 45751972399.09     Validation Loss: 102914312679.81\n",
      "Iteration  724  ->  Training Loss: 45746575358.37     Validation Loss: 102910178931.08\n",
      "Iteration  725  ->  Training Loss: 45741182250.77     Validation Loss: 102906009943.86\n",
      "Iteration  726  ->  Training Loss: 45735793070.47     Validation Loss: 102901805914.98\n",
      "Iteration  727  ->  Training Loss: 45730407811.68     Validation Loss: 102897567039.92\n",
      "Iteration  728  ->  Training Loss: 45725026468.67     Validation Loss: 102893293512.83\n",
      "Iteration  729  ->  Training Loss: 45719649035.71     Validation Loss: 102888985526.46\n",
      "Iteration  730  ->  Training Loss: 45714275507.11     Validation Loss: 102884643272.27\n",
      "Iteration  731  ->  Training Loss: 45708905877.21     Validation Loss: 102880266940.37\n",
      "Iteration  732  ->  Training Loss: 45703540140.38     Validation Loss: 102875856719.57\n",
      "Iteration  733  ->  Training Loss: 45698178291.01     Validation Loss: 102871412797.37\n",
      "Iteration  734  ->  Training Loss: 45692820323.52     Validation Loss: 102866935359.97\n",
      "Iteration  735  ->  Training Loss: 45687466232.38     Validation Loss: 102862424592.30\n",
      "Iteration  736  ->  Training Loss: 45682116012.06     Validation Loss: 102857880677.99\n",
      "Iteration  737  ->  Training Loss: 45676769657.06     Validation Loss: 102853303799.43\n",
      "Iteration  738  ->  Training Loss: 45671427161.93     Validation Loss: 102848694137.73\n",
      "Iteration  739  ->  Training Loss: 45666088521.22     Validation Loss: 102844051872.78\n",
      "Iteration  740  ->  Training Loss: 45660753729.52     Validation Loss: 102839377183.21\n",
      "Iteration  741  ->  Training Loss: 45655422781.44     Validation Loss: 102834670246.44\n",
      "Iteration  742  ->  Training Loss: 45650095671.62     Validation Loss: 102829931238.65\n",
      "Iteration  743  ->  Training Loss: 45644772394.72     Validation Loss: 102825160334.83\n",
      "Iteration  744  ->  Training Loss: 45639452945.44     Validation Loss: 102820357708.76\n",
      "Iteration  745  ->  Training Loss: 45634137318.48     Validation Loss: 102815523533.03\n",
      "Iteration  746  ->  Training Loss: 45628825508.59     Validation Loss: 102810657979.06\n",
      "Iteration  747  ->  Training Loss: 45623517510.53     Validation Loss: 102805761217.09\n",
      "Iteration  748  ->  Training Loss: 45618213319.08     Validation Loss: 102800833416.17\n",
      "Iteration  749  ->  Training Loss: 45612912929.05     Validation Loss: 102795874744.24\n",
      "Iteration  750  ->  Training Loss: 45607616335.28     Validation Loss: 102790885368.07\n",
      "Iteration  751  ->  Training Loss: 45602323532.63     Validation Loss: 102785865453.29\n",
      "Iteration  752  ->  Training Loss: 45597034515.97     Validation Loss: 102780815164.40\n",
      "Iteration  753  ->  Training Loss: 45591749280.20     Validation Loss: 102775734664.79\n",
      "Iteration  754  ->  Training Loss: 45586467820.25     Validation Loss: 102770624116.73\n",
      "Iteration  755  ->  Training Loss: 45581190131.07     Validation Loss: 102765483681.39\n",
      "Iteration  756  ->  Training Loss: 45575916207.62     Validation Loss: 102760313518.85\n",
      "Iteration  757  ->  Training Loss: 45570646044.89     Validation Loss: 102755113788.09\n",
      "Iteration  758  ->  Training Loss: 45565379637.90     Validation Loss: 102749884647.02\n",
      "Iteration  759  ->  Training Loss: 45560116981.68     Validation Loss: 102744626252.48\n",
      "Iteration  760  ->  Training Loss: 45554858071.28     Validation Loss: 102739338760.24\n",
      "Iteration  761  ->  Training Loss: 45549602901.77     Validation Loss: 102734022325.02\n",
      "Iteration  762  ->  Training Loss: 45544351468.25     Validation Loss: 102728677100.50\n",
      "Iteration  763  ->  Training Loss: 45539103765.84     Validation Loss: 102723303239.32\n",
      "Iteration  764  ->  Training Loss: 45533859789.66     Validation Loss: 102717900893.08\n",
      "Iteration  765  ->  Training Loss: 45528619534.88     Validation Loss: 102712470212.37\n",
      "Iteration  766  ->  Training Loss: 45523382996.67     Validation Loss: 102707011346.75\n",
      "Iteration  767  ->  Training Loss: 45518150170.22     Validation Loss: 102701524444.78\n",
      "Iteration  768  ->  Training Loss: 45512921050.74     Validation Loss: 102696009654.04\n",
      "Iteration  769  ->  Training Loss: 45507695633.47     Validation Loss: 102690467121.08\n",
      "Iteration  770  ->  Training Loss: 45502473913.66     Validation Loss: 102684896991.50\n",
      "Iteration  771  ->  Training Loss: 45497255886.58     Validation Loss: 102679299409.90\n",
      "Iteration  772  ->  Training Loss: 45492041547.50     Validation Loss: 102673674519.92\n",
      "Iteration  773  ->  Training Loss: 45486830891.75     Validation Loss: 102668022464.25\n",
      "Iteration  774  ->  Training Loss: 45481623914.64     Validation Loss: 102662343384.59\n",
      "Iteration  775  ->  Training Loss: 45476420611.52     Validation Loss: 102656637421.74\n",
      "Iteration  776  ->  Training Loss: 45471220977.74     Validation Loss: 102650904715.52\n",
      "Iteration  777  ->  Training Loss: 45466025008.69     Validation Loss: 102645145404.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  778  ->  Training Loss: 45460832699.75     Validation Loss: 102639359627.65\n",
      "Iteration  779  ->  Training Loss: 45455644046.33     Validation Loss: 102633547521.04\n",
      "Iteration  780  ->  Training Loss: 45450459043.88     Validation Loss: 102627709221.14\n",
      "Iteration  781  ->  Training Loss: 45445277687.82     Validation Loss: 102621844863.19\n",
      "Iteration  782  ->  Training Loss: 45440099973.63     Validation Loss: 102615954581.52\n",
      "Iteration  783  ->  Training Loss: 45434925896.77     Validation Loss: 102610038509.59\n",
      "Iteration  784  ->  Training Loss: 45429755452.76     Validation Loss: 102604096779.97\n",
      "Iteration  785  ->  Training Loss: 45424588637.09     Validation Loss: 102598129524.33\n",
      "Iteration  786  ->  Training Loss: 45419425445.29     Validation Loss: 102592136873.50\n",
      "Iteration  787  ->  Training Loss: 45414265872.90     Validation Loss: 102586118957.42\n",
      "Iteration  788  ->  Training Loss: 45409109915.49     Validation Loss: 102580075905.19\n",
      "Iteration  789  ->  Training Loss: 45403957568.63     Validation Loss: 102574007845.04\n",
      "Iteration  790  ->  Training Loss: 45398808827.90     Validation Loss: 102567914904.37\n",
      "Iteration  791  ->  Training Loss: 45393663688.92     Validation Loss: 102561797209.73\n",
      "Iteration  792  ->  Training Loss: 45388522147.29     Validation Loss: 102555654886.86\n",
      "Iteration  793  ->  Training Loss: 45383384198.65     Validation Loss: 102549488060.64\n",
      "Iteration  794  ->  Training Loss: 45378249838.66     Validation Loss: 102543296855.15\n",
      "Iteration  795  ->  Training Loss: 45373119062.98     Validation Loss: 102537081393.67\n",
      "Iteration  796  ->  Training Loss: 45367991867.28     Validation Loss: 102530841798.63\n",
      "Iteration  797  ->  Training Loss: 45362868247.26     Validation Loss: 102524578191.71\n",
      "Iteration  798  ->  Training Loss: 45357748198.63     Validation Loss: 102518290693.75\n",
      "Iteration  799  ->  Training Loss: 45352631717.10     Validation Loss: 102511979424.84\n",
      "Iteration  800  ->  Training Loss: 45347518798.41     Validation Loss: 102505644504.25\n",
      "Iteration  801  ->  Training Loss: 45342409438.31     Validation Loss: 102499286050.49\n",
      "Iteration  802  ->  Training Loss: 45337303632.57     Validation Loss: 102492904181.31\n",
      "Iteration  803  ->  Training Loss: 45332201376.96     Validation Loss: 102486499013.68\n",
      "Iteration  804  ->  Training Loss: 45327102667.26     Validation Loss: 102480070663.80\n",
      "Iteration  805  ->  Training Loss: 45322007499.29     Validation Loss: 102473619247.14\n",
      "Iteration  806  ->  Training Loss: 45316915868.86     Validation Loss: 102467144878.41\n",
      "Iteration  807  ->  Training Loss: 45311827771.80     Validation Loss: 102460647671.58\n",
      "Iteration  808  ->  Training Loss: 45306743203.95     Validation Loss: 102454127739.88\n",
      "Iteration  809  ->  Training Loss: 45301662161.16     Validation Loss: 102447585195.80\n",
      "Iteration  810  ->  Training Loss: 45296584639.32     Validation Loss: 102441020151.12\n",
      "Iteration  811  ->  Training Loss: 45291510634.29     Validation Loss: 102434432716.89\n",
      "Iteration  812  ->  Training Loss: 45286440141.98     Validation Loss: 102427823003.45\n",
      "Iteration  813  ->  Training Loss: 45281373158.28     Validation Loss: 102421191120.43\n",
      "Iteration  814  ->  Training Loss: 45276309679.12     Validation Loss: 102414537176.75\n",
      "Iteration  815  ->  Training Loss: 45271249700.43     Validation Loss: 102407861280.64\n",
      "Iteration  816  ->  Training Loss: 45266193218.15     Validation Loss: 102401163539.63\n",
      "Iteration  817  ->  Training Loss: 45261140228.24     Validation Loss: 102394444060.56\n",
      "Iteration  818  ->  Training Loss: 45256090726.66     Validation Loss: 102387702949.59\n",
      "Iteration  819  ->  Training Loss: 45251044709.39     Validation Loss: 102380940312.21\n",
      "Iteration  820  ->  Training Loss: 45246002172.43     Validation Loss: 102374156253.22\n",
      "Iteration  821  ->  Training Loss: 45240963111.78     Validation Loss: 102367350876.76\n",
      "Iteration  822  ->  Training Loss: 45235927523.45     Validation Loss: 102360524286.32\n",
      "Iteration  823  ->  Training Loss: 45230895403.46     Validation Loss: 102353676584.71\n",
      "Iteration  824  ->  Training Loss: 45225866747.86     Validation Loss: 102346807874.11\n",
      "Iteration  825  ->  Training Loss: 45220841552.70     Validation Loss: 102339918256.03\n",
      "Iteration  826  ->  Training Loss: 45215819814.02     Validation Loss: 102333007831.36\n",
      "Iteration  827  ->  Training Loss: 45210801527.91     Validation Loss: 102326076700.32\n",
      "Iteration  828  ->  Training Loss: 45205786690.45     Validation Loss: 102319124962.53\n",
      "Iteration  829  ->  Training Loss: 45200775297.73     Validation Loss: 102312152716.97\n",
      "Iteration  830  ->  Training Loss: 45195767345.84     Validation Loss: 102305160061.98\n",
      "Iteration  831  ->  Training Loss: 45190762830.92     Validation Loss: 102298147095.31\n",
      "Iteration  832  ->  Training Loss: 45185761749.08     Validation Loss: 102291113914.08\n",
      "Iteration  833  ->  Training Loss: 45180764096.46     Validation Loss: 102284060614.79\n",
      "Iteration  834  ->  Training Loss: 45175769869.20     Validation Loss: 102276987293.36\n",
      "Iteration  835  ->  Training Loss: 45170779063.46     Validation Loss: 102269894045.08\n",
      "Iteration  836  ->  Training Loss: 45165791675.42     Validation Loss: 102262780964.67\n",
      "Iteration  837  ->  Training Loss: 45160807701.24     Validation Loss: 102255648146.25\n",
      "Iteration  838  ->  Training Loss: 45155827137.12     Validation Loss: 102248495683.34\n",
      "Iteration  839  ->  Training Loss: 45150849979.26     Validation Loss: 102241323668.90\n",
      "Iteration  840  ->  Training Loss: 45145876223.86     Validation Loss: 102234132195.29\n",
      "Iteration  841  ->  Training Loss: 45140905867.13     Validation Loss: 102226921354.32\n",
      "Iteration  842  ->  Training Loss: 45135938905.32     Validation Loss: 102219691237.20\n",
      "Iteration  843  ->  Training Loss: 45130975334.66     Validation Loss: 102212441934.59\n",
      "Iteration  844  ->  Training Loss: 45126015151.39     Validation Loss: 102205173536.61\n",
      "Iteration  845  ->  Training Loss: 45121058351.77     Validation Loss: 102197886132.79\n",
      "Iteration  846  ->  Training Loss: 45116104932.08     Validation Loss: 102190579812.12\n",
      "Iteration  847  ->  Training Loss: 45111154888.58     Validation Loss: 102183254663.04\n",
      "Iteration  848  ->  Training Loss: 45106208217.57     Validation Loss: 102175910773.46\n",
      "Iteration  849  ->  Training Loss: 45101264915.34     Validation Loss: 102168548230.73\n",
      "Iteration  850  ->  Training Loss: 45096324978.19     Validation Loss: 102161167121.66\n",
      "Iteration  851  ->  Training Loss: 45091388402.45     Validation Loss: 102153767532.56\n",
      "Iteration  852  ->  Training Loss: 45086455184.43     Validation Loss: 102146349549.18\n",
      "Iteration  853  ->  Training Loss: 45081525320.48     Validation Loss: 102138913256.76\n",
      "Iteration  854  ->  Training Loss: 45076598806.92     Validation Loss: 102131458740.02\n",
      "Iteration  855  ->  Training Loss: 45071675640.11     Validation Loss: 102123986083.15\n",
      "Iteration  856  ->  Training Loss: 45066755816.42     Validation Loss: 102116495369.84\n",
      "Iteration  857  ->  Training Loss: 45061839332.22     Validation Loss: 102108986683.27\n",
      "Iteration  858  ->  Training Loss: 45056926183.87     Validation Loss: 102101460106.12\n",
      "Iteration  859  ->  Training Loss: 45052016367.78     Validation Loss: 102093915720.56\n",
      "Iteration  860  ->  Training Loss: 45047109880.33     Validation Loss: 102086353608.27\n",
      "Iteration  861  ->  Training Loss: 45042206717.93     Validation Loss: 102078773850.42\n",
      "Iteration  862  ->  Training Loss: 45037306877.00     Validation Loss: 102071176527.71\n",
      "Iteration  863  ->  Training Loss: 45032410353.95     Validation Loss: 102063561720.35\n",
      "Iteration  864  ->  Training Loss: 45027517145.22     Validation Loss: 102055929508.06\n",
      "Iteration  865  ->  Training Loss: 45022627247.25     Validation Loss: 102048279970.08\n",
      "Iteration  866  ->  Training Loss: 45017740656.49     Validation Loss: 102040613185.19\n",
      "Iteration  867  ->  Training Loss: 45012857369.38     Validation Loss: 102032929231.67\n",
      "Iteration  868  ->  Training Loss: 45007977382.41     Validation Loss: 102025228187.37\n",
      "Iteration  869  ->  Training Loss: 45003100692.03     Validation Loss: 102017510129.64\n",
      "Iteration  870  ->  Training Loss: 44998227294.73     Validation Loss: 102009775135.39\n",
      "Iteration  871  ->  Training Loss: 44993357187.01     Validation Loss: 102002023281.06\n",
      "Iteration  872  ->  Training Loss: 44988490365.35     Validation Loss: 101994254642.65\n",
      "Iteration  873  ->  Training Loss: 44983626826.27     Validation Loss: 101986469295.69\n",
      "Iteration  874  ->  Training Loss: 44978766566.28     Validation Loss: 101978667315.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  875  ->  Training Loss: 44973909581.89     Validation Loss: 101970848776.05\n",
      "Iteration  876  ->  Training Loss: 44969055869.65     Validation Loss: 101963013752.22\n",
      "Iteration  877  ->  Training Loss: 44964205426.09     Validation Loss: 101955162317.57\n",
      "Iteration  878  ->  Training Loss: 44959358247.75     Validation Loss: 101947294545.41\n",
      "Iteration  879  ->  Training Loss: 44954514331.19     Validation Loss: 101939410508.66\n",
      "Iteration  880  ->  Training Loss: 44949673672.97     Validation Loss: 101931510279.80\n",
      "Iteration  881  ->  Training Loss: 44944836269.66     Validation Loss: 101923593930.86\n",
      "Iteration  882  ->  Training Loss: 44940002117.83     Validation Loss: 101915661533.48\n",
      "Iteration  883  ->  Training Loss: 44935171214.07     Validation Loss: 101907713158.87\n",
      "Iteration  884  ->  Training Loss: 44930343554.98     Validation Loss: 101899748877.83\n",
      "Iteration  885  ->  Training Loss: 44925519137.15     Validation Loss: 101891768760.73\n",
      "Iteration  886  ->  Training Loss: 44920697957.19     Validation Loss: 101883772877.55\n",
      "Iteration  887  ->  Training Loss: 44915880011.71     Validation Loss: 101875761297.85\n",
      "Iteration  888  ->  Training Loss: 44911065297.34     Validation Loss: 101867734090.80\n",
      "Iteration  889  ->  Training Loss: 44906253810.71     Validation Loss: 101859691325.16\n",
      "Iteration  890  ->  Training Loss: 44901445548.45     Validation Loss: 101851633069.30\n",
      "Iteration  891  ->  Training Loss: 44896640507.22     Validation Loss: 101843559391.18\n",
      "Iteration  892  ->  Training Loss: 44891838683.65     Validation Loss: 101835470358.40\n",
      "Iteration  893  ->  Training Loss: 44887040074.41     Validation Loss: 101827366038.13\n",
      "Iteration  894  ->  Training Loss: 44882244676.16     Validation Loss: 101819246497.19\n",
      "Iteration  895  ->  Training Loss: 44877452485.59     Validation Loss: 101811111802.00\n",
      "Iteration  896  ->  Training Loss: 44872663499.37     Validation Loss: 101802962018.60\n",
      "Iteration  897  ->  Training Loss: 44867877714.18     Validation Loss: 101794797212.65\n",
      "Iteration  898  ->  Training Loss: 44863095126.73     Validation Loss: 101786617449.46\n",
      "Iteration  899  ->  Training Loss: 44858315733.71     Validation Loss: 101778422793.93\n",
      "Iteration  900  ->  Training Loss: 44853539531.83     Validation Loss: 101770213310.63\n",
      "Iteration  901  ->  Training Loss: 44848766517.82     Validation Loss: 101761989063.73\n",
      "Iteration  902  ->  Training Loss: 44843996688.38     Validation Loss: 101753750117.06\n",
      "Iteration  903  ->  Training Loss: 44839230040.26     Validation Loss: 101745496534.07\n",
      "Iteration  904  ->  Training Loss: 44834466570.19     Validation Loss: 101737228377.88\n",
      "Iteration  905  ->  Training Loss: 44829706274.91     Validation Loss: 101728945711.23\n",
      "Iteration  906  ->  Training Loss: 44824949151.17     Validation Loss: 101720648596.50\n",
      "Iteration  907  ->  Training Loss: 44820195195.72     Validation Loss: 101712337095.76\n",
      "Iteration  908  ->  Training Loss: 44815444405.34     Validation Loss: 101704011270.69\n",
      "Iteration  909  ->  Training Loss: 44810696776.80     Validation Loss: 101695671182.65\n",
      "Iteration  910  ->  Training Loss: 44805952306.86     Validation Loss: 101687316892.64\n",
      "Iteration  911  ->  Training Loss: 44801210992.32     Validation Loss: 101678948461.34\n",
      "Iteration  912  ->  Training Loss: 44796472829.96     Validation Loss: 101670565949.08\n",
      "Iteration  913  ->  Training Loss: 44791737816.59     Validation Loss: 101662169415.85\n",
      "Iteration  914  ->  Training Loss: 44787005948.99     Validation Loss: 101653758921.31\n",
      "Iteration  915  ->  Training Loss: 44782277224.00     Validation Loss: 101645334524.81\n",
      "Iteration  916  ->  Training Loss: 44777551638.41     Validation Loss: 101636896285.35\n",
      "Iteration  917  ->  Training Loss: 44772829189.06     Validation Loss: 101628444261.60\n",
      "Iteration  918  ->  Training Loss: 44768109872.77     Validation Loss: 101619978511.93\n",
      "Iteration  919  ->  Training Loss: 44763393686.38     Validation Loss: 101611499094.38\n",
      "Iteration  920  ->  Training Loss: 44758680626.73     Validation Loss: 101603006066.65\n",
      "Iteration  921  ->  Training Loss: 44753970690.67     Validation Loss: 101594499486.17\n",
      "Iteration  922  ->  Training Loss: 44749263875.05     Validation Loss: 101585979410.01\n",
      "Iteration  923  ->  Training Loss: 44744560176.74     Validation Loss: 101577445894.96\n",
      "Iteration  924  ->  Training Loss: 44739859592.60     Validation Loss: 101568898997.48\n",
      "Iteration  925  ->  Training Loss: 44735162119.51     Validation Loss: 101560338773.74\n",
      "Iteration  926  ->  Training Loss: 44730467754.34     Validation Loss: 101551765279.60\n",
      "Iteration  927  ->  Training Loss: 44725776493.99     Validation Loss: 101543178570.60\n",
      "Iteration  928  ->  Training Loss: 44721088335.34     Validation Loss: 101534578702.01\n",
      "Iteration  929  ->  Training Loss: 44716403275.29     Validation Loss: 101525965728.79\n",
      "Iteration  930  ->  Training Loss: 44711721310.75     Validation Loss: 101517339705.60\n",
      "Iteration  931  ->  Training Loss: 44707042438.62     Validation Loss: 101508700686.80\n",
      "Iteration  932  ->  Training Loss: 44702366655.83     Validation Loss: 101500048726.48\n",
      "Iteration  933  ->  Training Loss: 44697693959.29     Validation Loss: 101491383878.42\n",
      "Iteration  934  ->  Training Loss: 44693024345.94     Validation Loss: 101482706196.11\n",
      "Iteration  935  ->  Training Loss: 44688357812.70     Validation Loss: 101474015732.79\n",
      "Iteration  936  ->  Training Loss: 44683694356.53     Validation Loss: 101465312541.37\n",
      "Iteration  937  ->  Training Loss: 44679033974.35     Validation Loss: 101456596674.52\n",
      "Iteration  938  ->  Training Loss: 44674376663.14     Validation Loss: 101447868184.60\n",
      "Iteration  939  ->  Training Loss: 44669722419.83     Validation Loss: 101439127123.71\n",
      "Iteration  940  ->  Training Loss: 44665071241.41     Validation Loss: 101430373543.66\n",
      "Iteration  941  ->  Training Loss: 44660423124.84     Validation Loss: 101421607496.02\n",
      "Iteration  942  ->  Training Loss: 44655778067.09     Validation Loss: 101412829032.05\n",
      "Iteration  943  ->  Training Loss: 44651136065.14     Validation Loss: 101404038202.75\n",
      "Iteration  944  ->  Training Loss: 44646497115.99     Validation Loss: 101395235058.88\n",
      "Iteration  945  ->  Training Loss: 44641861216.62     Validation Loss: 101386419650.91\n",
      "Iteration  946  ->  Training Loss: 44637228364.03     Validation Loss: 101377592029.04\n",
      "Iteration  947  ->  Training Loss: 44632598555.23     Validation Loss: 101368752243.23\n",
      "Iteration  948  ->  Training Loss: 44627971787.23     Validation Loss: 101359900343.17\n",
      "Iteration  949  ->  Training Loss: 44623348057.04     Validation Loss: 101351036378.30\n",
      "Iteration  950  ->  Training Loss: 44618727361.68     Validation Loss: 101342160397.79\n",
      "Iteration  951  ->  Training Loss: 44614109698.19     Validation Loss: 101333272450.56\n",
      "Iteration  952  ->  Training Loss: 44609495063.58     Validation Loss: 101324372585.29\n",
      "Iteration  953  ->  Training Loss: 44604883454.91     Validation Loss: 101315460850.40\n",
      "Iteration  954  ->  Training Loss: 44600274869.21     Validation Loss: 101306537294.06\n",
      "Iteration  955  ->  Training Loss: 44595669303.53     Validation Loss: 101297601964.19\n",
      "Iteration  956  ->  Training Loss: 44591066754.93     Validation Loss: 101288654908.49\n",
      "Iteration  957  ->  Training Loss: 44586467220.46     Validation Loss: 101279696174.39\n",
      "Iteration  958  ->  Training Loss: 44581870697.20     Validation Loss: 101270725809.08\n",
      "Iteration  959  ->  Training Loss: 44577277182.21     Validation Loss: 101261743859.52\n",
      "Iteration  960  ->  Training Loss: 44572686672.56     Validation Loss: 101252750372.43\n",
      "Iteration  961  ->  Training Loss: 44568099165.35     Validation Loss: 101243745394.30\n",
      "Iteration  962  ->  Training Loss: 44563514657.65     Validation Loss: 101234728971.37\n",
      "Iteration  963  ->  Training Loss: 44558933146.57     Validation Loss: 101225701149.66\n",
      "Iteration  964  ->  Training Loss: 44554354629.19     Validation Loss: 101216661974.95\n",
      "Iteration  965  ->  Training Loss: 44549779102.61     Validation Loss: 101207611492.79\n",
      "Iteration  966  ->  Training Loss: 44545206563.96     Validation Loss: 101198549748.52\n",
      "Iteration  967  ->  Training Loss: 44540637010.34     Validation Loss: 101189476787.23\n",
      "Iteration  968  ->  Training Loss: 44536070438.86     Validation Loss: 101180392653.80\n",
      "Iteration  969  ->  Training Loss: 44531506846.66     Validation Loss: 101171297392.88\n",
      "Iteration  970  ->  Training Loss: 44526946230.87     Validation Loss: 101162191048.89\n",
      "Iteration  971  ->  Training Loss: 44522388588.61     Validation Loss: 101153073666.07\n",
      "Iteration  972  ->  Training Loss: 44517833917.02     Validation Loss: 101143945288.38\n",
      "Iteration  973  ->  Training Loss: 44513282213.26     Validation Loss: 101134805959.62\n",
      "Iteration  974  ->  Training Loss: 44508733474.47     Validation Loss: 101125655723.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  975  ->  Training Loss: 44504187697.80     Validation Loss: 101116494622.87\n",
      "Iteration  976  ->  Training Loss: 44499644880.43     Validation Loss: 101107322701.36\n",
      "Iteration  977  ->  Training Loss: 44495105019.50     Validation Loss: 101098140001.74\n",
      "Iteration  978  ->  Training Loss: 44490568112.20     Validation Loss: 101088946566.72\n",
      "Iteration  979  ->  Training Loss: 44486034155.69     Validation Loss: 101079742438.79\n",
      "Iteration  980  ->  Training Loss: 44481503147.16     Validation Loss: 101070527660.27\n",
      "Iteration  981  ->  Training Loss: 44476975083.79     Validation Loss: 101061302273.24\n",
      "Iteration  982  ->  Training Loss: 44472449962.78     Validation Loss: 101052066319.60\n",
      "Iteration  983  ->  Training Loss: 44467927781.32     Validation Loss: 101042819841.03\n",
      "Iteration  984  ->  Training Loss: 44463408536.60     Validation Loss: 101033562879.03\n",
      "Iteration  985  ->  Training Loss: 44458892225.84     Validation Loss: 101024295474.87\n",
      "Iteration  986  ->  Training Loss: 44454378846.25     Validation Loss: 101015017669.66\n",
      "Iteration  987  ->  Training Loss: 44449868395.04     Validation Loss: 101005729504.29\n",
      "Iteration  988  ->  Training Loss: 44445360869.42     Validation Loss: 100996431019.46\n",
      "Iteration  989  ->  Training Loss: 44440856266.63     Validation Loss: 100987122255.66\n",
      "Iteration  990  ->  Training Loss: 44436354583.90     Validation Loss: 100977803253.23\n",
      "Iteration  991  ->  Training Loss: 44431855818.46     Validation Loss: 100968474052.27\n",
      "Iteration  992  ->  Training Loss: 44427359967.55     Validation Loss: 100959134692.73\n",
      "Iteration  993  ->  Training Loss: 44422867028.41     Validation Loss: 100949785214.34\n",
      "Iteration  994  ->  Training Loss: 44418376998.30     Validation Loss: 100940425656.66\n",
      "Iteration  995  ->  Training Loss: 44413889874.46     Validation Loss: 100931056059.07\n",
      "Iteration  996  ->  Training Loss: 44409405654.16     Validation Loss: 100921676460.75\n",
      "Iteration  997  ->  Training Loss: 44404924334.67     Validation Loss: 100912286900.71\n",
      "Iteration  998  ->  Training Loss: 44400445913.24     Validation Loss: 100902887417.77\n",
      "Iteration  999  ->  Training Loss: 44395970387.15     Validation Loss: 100893478050.56\n",
      "b,w found by gradient descent: 175751.90, [[-8087.681258310568]\n",
      " [-23657.45634949141]\n",
      " [364868.5030954962]]\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "#initial_w = np.zeros(X_train.shape[0])\n",
    "initial_w = np.random.rand(X_train.shape[1],1)\n",
    "#initial_w =initial_w.reshape(X_train.shape[0],1)\n",
    "initial_b = 5.0\n",
    "print(initial_w)\n",
    "\n",
    "iterations = 1000\n",
    "alpha = 0.01\n",
    "\n",
    "\n",
    "w_final, b_final = Gradient_Descent(initial_w, initial_b, X_train, Y_train, X_valid,Y_valid, iterations,alpha )\n",
    "print(f\"b,w found by gradient descent: {b_final.item():0.2f}, {w_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee0ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a6ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 398095065324.66504 \n",
      "(359, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11040\\3017757641.py:3: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Test Loss: {float(test_loss)} \")\n"
     ]
    }
   ],
   "source": [
    "Y_test_predict=Predict(w_final,b_final,X_test)\n",
    "test_loss=Cost_Function(w_final,b_final,X_test,Y_test)\n",
    "print(f\"Test Loss: {float(test_loss)} \")\n",
    "print(Y_test_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f85edb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared: 19.873877798866303 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11040\\453783937.py:5: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"R-Squared: {float(R_Squared*100)} \")\n"
     ]
    }
   ],
   "source": [
    "test_mean=np.mean(Y_test)\n",
    "mse_total= np.sum((Y_test-test_mean)**2)\n",
    "mse_res = np.sum((Y_test-Y_test_predict)**2)\n",
    "R_Squared=1-(mse_res/mse_total)\n",
    "print(f\"R-Squared: {float(R_Squared)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2b5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
